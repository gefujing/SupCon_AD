{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, \n",
    "    confusion_matrix, roc_curve, precision_recall_curve, \n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*dropout_adj.*\", module=\"torch_geometric\")\n",
    "\n",
    "# 设置matplotlib中文字体和样式\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "plt.rcParams['figure.titlesize'] = 13\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置和路径设置\n",
    "EXPR_FILE = \"/home/fujing/ad_ssl/0_rawdata/exp_mono.csv\"\n",
    "LABEL_FILE = \"/home/fujing/ad_ssl/0_rawdata/ph_mono.csv\"\n",
    "SAMPLE_ID_COL = \"id\"\n",
    "LABEL_COL = \"group\"\n",
    "\n",
    "OUT_DIR = \"/home/fujing/ad_ssl/3_ssl_model\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2876870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数配置\n",
    "CONFIG = {\n",
    "    # 基础参数\n",
    "    \"RANDOM_SEED\": 2025,\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"MAX_EPOCHS\": 100,\n",
    "    \"PATIENCE\": 30,\n",
    "\n",
    "    # 预处理参数       \n",
    "    \"SELECT_TOP_VAR\": 3000,\n",
    "    \"LOG1P\": True,\n",
    "    \"Z_SCORE_PER_GENE\": True,\n",
    "    \"Z_SCORE_PER_SAMPLE\": True, \n",
    "\n",
    "    # 数据增强\n",
    "    \"USE_MIXUP\": True,\n",
    "    \"MIXUP_ALPHA\": 0.2,\n",
    "    \"USE_GAUSSIAN_NOISE\": True,\n",
    "    \"NOISE_STD\": 0.05,\n",
    "    \"USE_FEATURE_DROPOUT\": True,\n",
    "    \"FEATURE_DROPOUT_RATE\": 0.1,\n",
    "    \n",
    "    # 损失函数\n",
    "    \"USE_FOCAL_LOSS\": True,\n",
    "    \"FOCAL_ALPHA\": 0.25,\n",
    "    \"FOCAL_GAMMA\": 2.0,\n",
    "    \"USE_LABEL_SMOOTHING\": True,\n",
    "    \"LABEL_SMOOTHING\": 0.1,\n",
    "    \n",
    "    # 对比学习\n",
    "    \"SUPCON_WEIGHT\": 0.5,\n",
    "    \"SUPCON_TEMP\": 0.05,\n",
    "    \n",
    "    # 训练策略\n",
    "    \"LR\": 1e-3,\n",
    "    \"MIN_LR\": 1e-6,\n",
    "    \"WEIGHT_DECAY\": 1e-5,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"WARMUP_EPOCHS\": 5,\n",
    "    \"USE_SWA\": False,\n",
    "    \"SWA_START\": 70,\n",
    "    \n",
    "    # 模型架构\n",
    "    \"HIDDEN_DIMS\": [512, 256, 128, 64],\n",
    "    \"DROPOUT\": 0.4,\n",
    "    \"PROJ_DIM\": 256,\n",
    "    \"NORM_FEATS\": True,  \n",
    "    \"USE_BATCH_NORM\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def _detect_delimiter(path: str):\n",
    "    if path.endswith((\".tsv\", \".txt\")):\n",
    "        return \"\\t\"\n",
    "    return \",\"\n",
    "\n",
    "\n",
    "def read_expression(expr_file: str) -> pd.DataFrame:\n",
    "    delim = \"\\t\" if expr_file.endswith((\".tsv\", \".txt\")) else \",\"\n",
    "    df = pd.read_csv(expr_file, sep=delim, header=0, dtype=str, low_memory=False)\n",
    "\n",
    "    df.columns = df.columns.astype(str).str.strip().str.replace('\"', \"\")\n",
    "    first_col = df.columns[0]\n",
    "\n",
    "    is_numeric_firstcol = df[first_col].str.replace('\"', \"\").str.match(r\"^-?\\d+(\\.\\d+)?$\").all()\n",
    "    if not is_numeric_firstcol:\n",
    "        df[first_col] = df[first_col].astype(str).str.strip().str.replace('\"', \"\")\n",
    "        df = df.set_index(first_col)\n",
    "\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c].astype(str).str.strip().str.replace('\"', \"\"), errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(how=\"all\").fillna(0.0)\n",
    "    df = df.loc[(df.sum(axis=1) > 0)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_labels(label_file: str, sample_col: str, label_col: str) -> pd.Series:\n",
    "    delim = _detect_delimiter(label_file)\n",
    "    tab = pd.read_csv(label_file, sep=delim, header=0)\n",
    "    tab = tab[[sample_col, label_col]].dropna()\n",
    "    y_raw = tab[label_col].astype(str).str.upper().str.strip()\n",
    "    if set(y_raw.unique()) <= {\"0\",\"1\"}:\n",
    "        y_num = y_raw.astype(int)\n",
    "    else:\n",
    "        y_num = y_raw.replace({\"AD\":1, \"CASE\":1, \"PATIENT\":1, \"POS\":1,\n",
    "                               \"HC\":0, \"CTRL\":0, \"CONTROL\":0, \"NEG\":0}).astype(int)\n",
    "    y = pd.Series(y_num.values, index=tab[sample_col].astype(str).values, name=\"label\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def align_expr_labels(expr: pd.DataFrame, labels: pd.Series=None):\n",
    "    samples = expr.columns.astype(str)\n",
    "    if labels is None:\n",
    "        y = pd.Series(np.zeros(len(samples), dtype=int), index=samples, name=\"label\")\n",
    "    else:\n",
    "        labels.index = labels.index.astype(str)\n",
    "        common = samples.intersection(labels.index)\n",
    "        expr = expr[common]\n",
    "        y = labels.loc[common]\n",
    "    return expr, y\n",
    "\n",
    "\n",
    "def select_top_var_genes(expr: pd.DataFrame, n_top: int=None) -> pd.DataFrame:\n",
    "    if not n_top or n_top >= expr.shape[0]:\n",
    "        return expr\n",
    "    var = expr.var(axis=1)\n",
    "    top_idx = var.sort_values(ascending=False).head(n_top).index\n",
    "    return expr.loc[top_idx]\n",
    "\n",
    "\n",
    "def preprocess_expr(expr: pd.DataFrame, log1p=True, zscore_per_gene=True, stats=None) -> pd.DataFrame:\n",
    "    X = expr.copy()\n",
    "    if log1p:\n",
    "        X = np.log1p(X)\n",
    "    if zscore_per_gene:\n",
    "        if stats is not None:\n",
    "            mean, std = stats\n",
    "        else:\n",
    "            mean = X.mean(axis=1)\n",
    "            std  = X.std(axis=1, ddof=0)\n",
    "        std = std.replace(0, 1.0)\n",
    "        X = (X.sub(mean, axis=0)).div(std, axis=0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_and_preprocess_raw_data(expr_file, label_file, sample_id_col, label_col, config=CONFIG):\n",
    "    \n",
    "    expr_df = read_expression(expr_file)\n",
    "    labels_series = read_labels(label_file, sample_id_col, label_col)\n",
    "    \n",
    "    expr_aligned, labels_aligned = align_expr_labels(expr_df, labels_series)\n",
    "\n",
    "    expr_top_var = select_top_var_genes(expr_aligned)\n",
    "\n",
    "    expr_for_stats = np.log1p(expr_top_var) if config[\"LOG1P\"] else expr_top_var\n",
    "    mean_all = expr_for_stats.mean(axis=1)\n",
    "    std_all  = expr_for_stats.std(axis=1, ddof=0).replace(0, 1.0) # ddof=0 avoids NaN for single columns\n",
    "    stats = (mean_all, std_all)\n",
    "\n",
    "    expr_processed = preprocess_expr(expr_top_var, log1p=config[\"LOG1P\"], zscore_per_gene=config[\"Z_SCORE_PER_GENE\"], stats=stats)\n",
    "\n",
    "    features_matrix = expr_processed.T.values # Shape: (n_samples, n_genes)\n",
    "    labels_vector = labels_aligned.values    # Shape: (n_samples,)\n",
    "    gene_names = expr_processed.index.tolist()  # 基因名称列表\n",
    "\n",
    "    print(f\"Final features matrix shape: {features_matrix.shape}\")\n",
    "    print(f\"Final labels vector shape: {labels_vector.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels_vector)}\")\n",
    "    print(f\"Number of genes: {len(gene_names)}\")\n",
    "    \n",
    "    return features_matrix, labels_vector, gene_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedine model and training\n",
    "class GeneDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class GeneClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes=2,\n",
    "                 dropout_rate=0.3, use_batch_norm=True, proj_dim=256):\n",
    "        super(GeneClassifier, self).__init__()\n",
    "\n",
    "        feat_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            feat_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if use_batch_norm:\n",
    "                feat_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            feat_layers.append(nn.ReLU())\n",
    "            feat_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        self.feature_extractor = nn.Sequential(*feat_layers)\n",
    "        self.feat_dim = prev_dim\n",
    "\n",
    "        self.classifier = nn.Linear(self.feat_dim, num_classes)\n",
    "\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, self.feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.feat_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_feats=False, return_proj=False):\n",
    "        feats = self.feature_extractor(x)        # (B, feat_dim)\n",
    "        logits = self.classifier(feats)          # (B, num_classes)\n",
    "        if return_proj:\n",
    "            z = self.proj_head(feats)            # (B, proj_dim)\n",
    "            return logits, feats, z\n",
    "        if return_feats:\n",
    "            return logits, feats\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature=0.07, eps=1e-8, normalize=True):\n",
    "        super().__init__()\n",
    "        self.tau = temperature\n",
    "        self.eps = eps\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, features: torch.Tensor, labels: torch.Tensor):\n",
    "\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "        if self.normalize:\n",
    "            features = torch.nn.functional.normalize(features, dim=1)\n",
    "\n",
    "        logits = torch.div(features @ features.t(), self.tau)\n",
    "\n",
    "        logits = logits - torch.eye(B, device=device) * 1e9  \n",
    "\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.t()).float().to(device)\n",
    "        mask = mask - torch.eye(B, device=device)\n",
    "\n",
    "        log_prob = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "\n",
    "        positives_per_sample = mask.sum(dim=1)  # (B,)\n",
    "        loss = -(mask * log_prob).sum(dim=1) / (positives_per_sample + self.eps)\n",
    "        valid = (positives_per_sample > 0).float()\n",
    "        denom = valid.sum() + self.eps\n",
    "        return (loss * valid).sum() / denom\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device, criterion):\n",
    "    \"\"\"Evaluate the model on a given dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Prob of positive class\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    if len(dataloader) == 0:\n",
    "        print(\"Warning: DataLoader is empty in evaluate function.\")\n",
    "        return float('nan'), float('nan'), float('nan'), float('nan'), [], [], []\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    auc = 0.0\n",
    "    f1 = 0.0\n",
    "    try:\n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "        else:\n",
    "            print(f\"Warning: Only one unique label ({np.unique(all_labels)}) in evaluation batch. Cannot calculate AUC/F1.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate AUC/F1: {e}\")\n",
    "\n",
    "    return avg_loss, acc, auc, f1, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataAugmentation:\n",
    "class DataAugmentation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def mixup(self, x, y, alpha=0.2):\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index]\n",
    "        y_a, y_b = y, y[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "    def gaussian_noise(self, x, std=0.05):\n",
    "        noise = torch.randn_like(x) * std\n",
    "        return x + noise\n",
    "    \n",
    "    def feature_dropout(self, x, dropout_rate=0.1):\n",
    "        mask = torch.bernoulli(torch.ones_like(x) * (1 - dropout_rate))\n",
    "        return x * mask\n",
    "    \n",
    "    def apply_augmentation(self, x, training=True):\n",
    "        if not training:\n",
    "            return x\n",
    "        \n",
    "        if self.config.get(\"USE_GAUSSIAN_NOISE\", False):\n",
    "            if np.random.rand() > 0.5: \n",
    "                x = self.gaussian_noise(x, self.config.get(\"NOISE_STD\", 0.05))\n",
    "        \n",
    "        if self.config.get(\"USE_FEATURE_DROPOUT\", False):\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = self.feature_dropout(x, self.config.get(\"FEATURE_DROPOUT_RATE\", 0.1))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \n",
    "    def __init__(self, smoothing=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_pred = nn.functional.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_pred)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        loss = -true_dist * log_pred\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight.unsqueeze(0)\n",
    "        \n",
    "        return loss.sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d81c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_model\n",
    "def train_model(train_loader, val_loader, input_dim, num_classes, device, \n",
    "                class_weights, config=CONFIG):\n",
    "    \n",
    "    model = GeneClassifier(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dims=config[\"HIDDEN_DIMS\"], \n",
    "        num_classes=num_classes, \n",
    "        dropout_rate=config[\"DROPOUT\"], \n",
    "        use_batch_norm=config[\"USE_BATCH_NORM\"], \n",
    "        proj_dim=config[\"PROJ_DIM\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    augmenter = DataAugmentation(config)\n",
    "    \n",
    "    if config[\"USE_FOCAL_LOSS\"]:\n",
    "        ce_criterion = FocalLoss(alpha=config[\"FOCAL_ALPHA\"], gamma=config[\"FOCAL_GAMMA\"])\n",
    "    elif config[\"USE_LABEL_SMOOTHING\"]:\n",
    "        cw = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float, device=device)\n",
    "        ce_criterion = LabelSmoothingCrossEntropy(smoothing=config[\"LABEL_SMOOTHING\"], weight=cw)\n",
    "    else:\n",
    "        cw = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float, device=device)\n",
    "        ce_criterion = nn.CrossEntropyLoss(weight=cw)\n",
    "    \n",
    "    supcon_criterion = SupConLoss(temperature=config[\"SUPCON_TEMP\"], normalize=config[\"NORM_FEATS\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"LR\"], weight_decay=config[\"WEIGHT_DECAY\"])\n",
    "    \n",
    "    def warmup_lambda(epoch):\n",
    "        if epoch < config[\"WARMUP_EPOCHS\"]:\n",
    "            return (epoch + 1) / config[\"WARMUP_EPOCHS\"]\n",
    "        return 1.0\n",
    "    \n",
    "    warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config[\"MAX_EPOCHS\"] - config[\"WARMUP_EPOCHS\"], eta_min=config[\"MIN_LR\"]\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_auc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, config[\"MAX_EPOCHS\"] + 1):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_correct = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            bs = batch_labels.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if config[\"USE_MIXUP\"] and np.random.rand() > 0.5:\n",
    "                mixed_x, y_a, y_b, lam = augmenter.mixup(\n",
    "                    batch_features, batch_labels, alpha=config[\"MIXUP_ALPHA\"]\n",
    "                )\n",
    "                mixed_x = augmenter.apply_augmentation(mixed_x, training=True)\n",
    "                \n",
    "                logits, _, z = model(mixed_x, return_proj=True)\n",
    "                \n",
    "                # Mixup loss\n",
    "                ce_loss = mixup_criterion(ce_criterion, logits, y_a, y_b, lam)\n",
    "                sc_loss = supcon_criterion(z, batch_labels)\n",
    "            else:\n",
    "                aug_x = augmenter.apply_augmentation(batch_features, training=True)\n",
    "                logits, _, z = model(aug_x, return_proj=True)\n",
    "                \n",
    "                ce_loss = ce_criterion(logits, batch_labels)\n",
    "                sc_loss = supcon_criterion(z, batch_labels)\n",
    "            \n",
    "            loss = ce_loss + config[\"SUPCON_WEIGHT\"] * sc_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item() * bs\n",
    "            train_correct += (logits.argmax(dim=1) == batch_labels).sum().item()\n",
    "            train_count += bs\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / max(1, train_count)\n",
    "        train_acc = train_correct / max(1, train_count)\n",
    "        \n",
    "        val_loss, val_acc, val_auc, val_f1, _, _, _ = evaluate_model(\n",
    "            model, val_loader, device, nn.CrossEntropyLoss()\n",
    "        )\n",
    "        \n",
    "        if epoch <= config[\"WARMUP_EPOCHS\"]:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            cosine_scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] LR: {current_lr:.6f} | \"\n",
    "                  f\"TrainLoss: {avg_train_loss:.4f}, TrainAcc: {train_acc:.3f} | \"\n",
    "                  f\"ValLoss: {val_loss:.4f}, ValAcc: {val_acc:.3f}, ValAUC: {val_auc:.3f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= config[\"PATIENCE\"]:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_loss, best_val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89778c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define run_classification\n",
    "def run_improved_classification():\n",
    "    \n",
    "    seed_everything(CONFIG[\"RANDOM_SEED\"])\n",
    "    device = get_device()\n",
    "    \n",
    "    features, labels, _ = load_and_preprocess_raw_data(EXPR_FILE, LABEL_FILE, SAMPLE_ID_COL, LABEL_COL)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weight_dict = {int(cls): float(weight) for cls, weight in zip(classes, class_weights)}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=CONFIG[\"N_FOLDS\"], shuffle=True, random_state=CONFIG[\"RANDOM_SEED\"])\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(features, labels), 1):\n",
    "\n",
    "        X_train_fold, X_test_fold = features[train_idx], features[test_idx]\n",
    "        y_train_fold, y_test_fold = labels[train_idx], labels[test_idx]\n",
    "        \n",
    "        skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=CONFIG[\"RANDOM_SEED\"] + fold)\n",
    "        inner_splits = list(skf_inner.split(X_train_fold, y_train_fold))\n",
    "        inner_train_idx, inner_val_idx = inner_splits[0]\n",
    "        \n",
    "        X_tr, X_val = X_train_fold[inner_train_idx], X_train_fold[inner_val_idx]\n",
    "        y_tr, y_val = y_train_fold[inner_train_idx], y_train_fold[inner_val_idx]\n",
    "        \n",
    "        train_dataset = GeneDataset(X_tr, y_tr)\n",
    "        val_dataset = GeneDataset(X_val, y_val)\n",
    "        test_dataset = GeneDataset(X_test_fold, y_test_fold)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trained_model, best_val_loss, best_val_auc = train_model(\n",
    "            train_loader, val_loader, features.shape[1], n_classes, device, class_weight_dict\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        test_loss, test_acc, test_auc, test_f1, test_preds, test_labels, test_probs = evaluate_model(\n",
    "            trained_model, test_loader, device, nn.CrossEntropyLoss()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        fold_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"test_auc\": test_auc,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"best_val_auc\": best_val_auc,\n",
    "            \"training_time\": training_time,\n",
    "            \"y_true\": list(map(int, test_labels)),\n",
    "            \"y_prob\": list(map(float, test_probs)),\n",
    "            \"y_pred\": list(map(int, test_preds))\n",
    "        })\n",
    "        \n",
    "        fold_model_path = os.path.join(OUT_DIR, f\"improved_model_fold_{fold}.pth\")\n",
    "        torch.save(trained_model.state_dict(), fold_model_path)\n",
    "    \n",
    "    test_accs = [r[\"test_acc\"] for r in fold_results]\n",
    "    test_aucs = [r[\"test_auc\"] for r in fold_results]\n",
    "    test_f1s = [r[\"test_f1\"] for r in fold_results]\n",
    "    \n",
    "    mean_acc = np.mean(test_accs)\n",
    "    std_acc = np.std(test_accs)\n",
    "    mean_auc = np.mean(test_aucs)\n",
    "    std_auc = np.std(test_aucs)\n",
    "    mean_f1 = np.mean(test_f1s)\n",
    "    std_f1 = np.std(test_f1s)\n",
    "    \n",
    "    print(f\"Accuracy:  {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    print(f\"AUC:       {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    print(f\"F1-Score:  {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    summary = {\n",
    "        \"config\": CONFIG,\n",
    "        \"fold_results\": fold_results,\n",
    "        \"overall_metrics\": {\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_auc\": mean_auc,\n",
    "            \"std_auc\": std_auc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(OUT_DIR, \"improved_classification_results.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_model\n",
    "def train_ensemble_models(n_models=10):\n",
    "\n",
    "    device = get_device()\n",
    "    \n",
    "    features, labels, _ = load_and_preprocess_raw_data(EXPR_FILE, LABEL_FILE, SAMPLE_ID_COL, LABEL_COL)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weight_dict = {int(cls): float(weight) for cls, weight in zip(classes, class_weights)}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=CONFIG[\"N_FOLDS\"], shuffle=True, random_state=CONFIG[\"RANDOM_SEED\"])\n",
    "    \n",
    "    ensemble_fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(features, labels), 1):\n",
    "        \n",
    "        X_train_fold, X_test_fold = features[train_idx], features[test_idx]\n",
    "        y_train_fold, y_test_fold = labels[train_idx], labels[test_idx]\n",
    "        \n",
    "        fold_model_probs = []\n",
    "        fold_models = []\n",
    "        \n",
    "        for model_idx in range(n_models):\n",
    "            \n",
    "            seed_everything(CONFIG[\"RANDOM_SEED\"] + fold * 100 + model_idx)\n",
    "            \n",
    "            skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=CONFIG[\"RANDOM_SEED\"] + fold * 100 + model_idx)\n",
    "            inner_splits = list(skf_inner.split(X_train_fold, y_train_fold))\n",
    "            inner_train_idx, inner_val_idx = inner_splits[0]\n",
    "            \n",
    "            X_tr, X_val = X_train_fold[inner_train_idx], X_train_fold[inner_val_idx]\n",
    "            y_tr, y_val = y_train_fold[inner_train_idx], y_train_fold[inner_val_idx]\n",
    "            \n",
    "            train_dataset = GeneDataset(X_tr, y_tr)\n",
    "            val_dataset = GeneDataset(X_val, y_val)\n",
    "            test_dataset = GeneDataset(X_test_fold, y_test_fold)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=False)\n",
    "            \n",
    "            trained_model, _, _ = train_model(\n",
    "                train_loader, val_loader, features.shape[1], n_classes, device, class_weight_dict\n",
    "            )\n",
    "\n",
    "            \n",
    "            _, _, _, _, _, _, test_probs = evaluate_model(\n",
    "                trained_model, test_loader, device, nn.CrossEntropyLoss()\n",
    "            )\n",
    "            fold_model_probs.append(test_probs)\n",
    "            fold_models.append(trained_model)\n",
    "        \n",
    "        ensemble_probs = np.mean(fold_model_probs, axis=0)\n",
    "        ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
    "        \n",
    "        test_acc = accuracy_score(y_test_fold, ensemble_preds)\n",
    "        test_auc = roc_auc_score(y_test_fold, ensemble_probs)\n",
    "        test_f1 = f1_score(y_test_fold, ensemble_preds)\n",
    "        \n",
    "        print(f\"- Ensemble AUC: {test_auc:.4f}\")\n",
    "        print(f\"- Ensemble Acc: {test_acc:.4f}\")\n",
    "        print(f\"- Ensemble F1:  {test_f1:.4f}\")\n",
    "        \n",
    "        individual_aucs = []\n",
    "        for i, probs in enumerate(fold_model_probs):\n",
    "            auc = roc_auc_score(y_test_fold, probs)\n",
    "            individual_aucs.append(auc)\n",
    "\n",
    "        ensemble_fold_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"test_auc\": test_auc,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"individual_aucs\": individual_aucs,\n",
    "            \"ensemble_prob\": ensemble_probs.tolist(),\n",
    "            \"ensemble_pred\": ensemble_preds.tolist(),\n",
    "            \"y_true\": y_test_fold.tolist()\n",
    "        })\n",
    "        \n",
    "        for i, model in enumerate(fold_models):\n",
    "            model_path = os.path.join(OUT_DIR, f\"ensemble_fold{fold}_model{i+1}.pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    ensemble_aucs = [r[\"test_auc\"] for r in ensemble_fold_results]\n",
    "    ensemble_accs = [r[\"test_acc\"] for r in ensemble_fold_results]\n",
    "    ensemble_f1s = [r[\"test_f1\"] for r in ensemble_fold_results]\n",
    "    \n",
    "    mean_auc = np.mean(ensemble_aucs)\n",
    "    std_auc = np.std(ensemble_aucs)\n",
    "    mean_acc = np.mean(ensemble_accs)\n",
    "    std_acc = np.std(ensemble_accs)\n",
    "    mean_f1 = np.mean(ensemble_f1s)\n",
    "    std_f1 = np.std(ensemble_f1s)\n",
    "    \n",
    "    print(f\"Ensemble Accuracy:  {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    print(f\"Ensemble AUC:       {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    print(f\"Ensemble F1-Score:  {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    # 保存结果\n",
    "    summary = {\n",
    "        \"n_models\": n_models,\n",
    "        \"fold_results\": ensemble_fold_results,\n",
    "        \"overall_metrics\": {\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_auc\": mean_auc,\n",
    "            \"std_auc\": std_auc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(OUT_DIR, f\"ensemble_{n_models}models_results.json\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模块 - 所有绘图函数\n",
    "def plot_three_metrics_separate_subplots(\n",
    "    df_summary: pd.DataFrame, \n",
    "    fold_results_dict: Dict,\n",
    "    figsize: Tuple[int, int] = (12, 4.5),\n",
    "    colors: Dict[str, str] = None,\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"\n",
    "    绘制三个指标（Accuracy, AUC, F1）的柱状图\n",
    "    \n",
    "    Returns:\n",
    "        fig, axes: matplotlib图形对象，可在notebook中进一步调整\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = {\"acc\": \"#4C72B0\", \"auc\": \"#55A868\", \"f1\": \"#C44E52\"}\n",
    "    \n",
    "    models = df_summary.index.tolist()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    # Accuracy\n",
    "    means_acc = df_summary[\"mean_acc\"].values\n",
    "    stds_acc = df_summary[\"std_acc\"].values\n",
    "    bars1 = ax1.bar(x, means_acc, yerr=stds_acc, capsize=6, alpha=0.9, color=colors[\"acc\"])\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        if model not in fold_results_dict:\n",
    "            continue\n",
    "        fold_results = fold_results_dict[model][\"fold_results\"]\n",
    "        vals = [fr.get(\"test_acc\") for fr in fold_results if \"test_acc\" in fr]\n",
    "        jitter = np.random.uniform(-0.06, 0.06, size=len(vals))\n",
    "        ax1.scatter(np.full(len(vals), x[j]) + jitter, vals, color=\"gray\", alpha=0.75, s=20, zorder=3)\n",
    "\n",
    "    for j, (m, s) in enumerate(zip(means_acc, stds_acc)):\n",
    "        if np.isnan(m): continue\n",
    "        ax1.text(x[j], m + s + 0.02, f\"{m:.3f}±{s:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=30, ha=\"right\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_ylim(0.5, 0.75)\n",
    "\n",
    "    # AUC\n",
    "    means_auc = df_summary[\"mean_auc\"].values\n",
    "    stds_auc = df_summary[\"std_auc\"].values\n",
    "    bars2 = ax2.bar(x, means_auc, yerr=stds_auc, capsize=6, alpha=0.9, color=colors[\"auc\"])\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        if model not in fold_results_dict:\n",
    "            continue\n",
    "        fold_results = fold_results_dict[model][\"fold_results\"]\n",
    "        vals = [fr.get(\"test_auc\") for fr in fold_results if \"test_auc\" in fr and fr[\"test_auc\"] is not None]\n",
    "        jitter = np.random.uniform(-0.06, 0.06, size=len(vals))\n",
    "        ax2.scatter(np.full(len(vals), x[j]) + jitter, vals, color=\"gray\", alpha=0.75, s=20, zorder=3)\n",
    "\n",
    "    for j, (m, s) in enumerate(zip(means_auc, stds_auc)):\n",
    "        if np.isnan(m): continue\n",
    "        ax2.text(x[j], m + s + 0.02, f\"{m:.3f}±{s:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    ax2.set_title(\"AUC\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models, rotation=30, ha=\"right\")\n",
    "    ax2.set_ylabel(\"AUC\")\n",
    "    ax2.set_ylim(0.6, 0.8)\n",
    "\n",
    "    # F1\n",
    "    means_f1 = df_summary[\"mean_f1\"].values\n",
    "    stds_f1 = df_summary[\"std_f1\"].values\n",
    "    bars3 = ax3.bar(x, means_f1, yerr=stds_f1, capsize=6, alpha=0.9, color=colors[\"f1\"])\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        if model not in fold_results_dict:\n",
    "            continue\n",
    "        fold_results = fold_results_dict[model][\"fold_results\"]\n",
    "        vals = [fr.get(\"test_f1\") for fr in fold_results if \"test_f1\" in fr and fr[\"test_f1\"] is not None]\n",
    "        jitter = np.random.uniform(-0.06, 0.06, size=len(vals))\n",
    "        ax3.scatter(np.full(len(vals), x[j]) + jitter, vals, color=\"gray\", alpha=0.75, s=20, zorder=3)\n",
    "\n",
    "    for j, (m, s) in enumerate(zip(means_f1, stds_f1)):\n",
    "        if np.isnan(m): continue\n",
    "        ax3.text(x[j], m + s + 0.02, f\"{m:.3f}±{s:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    ax3.set_title(\"F1-Score\")\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models, rotation=30, ha=\"right\")\n",
    "    ax3.set_ylabel(\"F1\")\n",
    "    ax3.set_ylim(0, 0.7)\n",
    "\n",
    "    fig.suptitle(\"Model Performance (mean ± sd with per-fold points)\", fontsize=14, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, (ax1, ax2, ax3)\n",
    "\n",
    "\n",
    "def _collect_curves_from_folds(fold_pack):\n",
    "    \"\"\"收集所有折的y_true和y_prob对\"\"\"\n",
    "    pairs = []\n",
    "    for fr in fold_pack.get(\"fold_results\", []):\n",
    "        y_true = fr.get(\"y_true\", None)\n",
    "        y_prob = fr.get(\"y_prob\") or fr.get(\"ensemble_prob\")\n",
    "        if y_true is None or y_prob is None:\n",
    "            continue\n",
    "        if len(set(y_true)) < 2:\n",
    "            continue\n",
    "        if len(y_true) != len(y_prob):\n",
    "            continue\n",
    "        pairs.append((np.array(y_true, dtype=int), np.array(y_prob, dtype=float)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _mean_std_curve_roc(pairs, n_points=101):\n",
    "    \"\"\"计算ROC曲线的均值和标准差\"\"\"\n",
    "    if not pairs:\n",
    "        return None\n",
    "    fpr_grid = np.linspace(0, 1, n_points)\n",
    "    tpr_mat = []\n",
    "    auc_list = []\n",
    "    for y, p in pairs:\n",
    "        fpr, tpr, _ = roc_curve(y, p)\n",
    "        tpr_interp = np.interp(fpr_grid, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0\n",
    "        tpr_mat.append(tpr_interp)\n",
    "        auc_list.append(roc_auc_score(y, p))\n",
    "    tpr_mat = np.vstack(tpr_mat)\n",
    "    mean_tpr = tpr_mat.mean(axis=0)\n",
    "    std_tpr = tpr_mat.std(axis=0)\n",
    "    mean_auc = float(np.mean(auc_list))\n",
    "    std_auc = float(np.std(auc_list))\n",
    "    return {\"x\": fpr_grid, \"mean\": mean_tpr, \"std\": std_tpr, \"mean_auc\": mean_auc, \"std_auc\": std_auc}\n",
    "\n",
    "\n",
    "def _mean_std_curve_pr(pairs, n_points=101):\n",
    "    \"\"\"计算PR曲线的均值和标准差\"\"\"\n",
    "    if not pairs:\n",
    "        return None\n",
    "    recall_grid = np.linspace(0, 1, n_points)\n",
    "    prec_mat = []\n",
    "    ap_list = []\n",
    "    for y, p in pairs:\n",
    "        precision, recall, _ = precision_recall_curve(y, p)\n",
    "        order = np.argsort(recall)\n",
    "        recall_sorted = recall[order]\n",
    "        precision_sorted = precision[order]\n",
    "        prec_interp = np.interp(recall_grid, recall_sorted, precision_sorted)\n",
    "        prec_mat.append(prec_interp)\n",
    "        ap_list.append(average_precision_score(y, p))\n",
    "    prec_mat = np.vstack(prec_mat)\n",
    "    mean_prec = prec_mat.mean(axis=0)\n",
    "    std_prec = prec_mat.std(axis=0)\n",
    "    mean_ap = float(np.mean(ap_list))\n",
    "    std_ap = float(np.std(ap_list))\n",
    "    return {\"x\": recall_grid, \"mean\": mean_prec, \"std\": std_prec, \"mean_ap\": mean_ap, \"std_ap\": std_ap}\n",
    "\n",
    "\n",
    "def plot_mean_roc_curve(\n",
    "    fold_results_dict: Dict,\n",
    "    figsize: Tuple[int, int] = (5, 5),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制平均ROC曲线\"\"\"\n",
    "    roc_data = {}\n",
    "    for model_name, pack in fold_results_dict.items():\n",
    "        pairs = _collect_curves_from_folds(pack)\n",
    "        if not pairs:\n",
    "            continue\n",
    "        roc_data[model_name] = _mean_std_curve_roc(pairs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for model_name, dat in roc_data.items():\n",
    "        if dat is None:\n",
    "            continue\n",
    "        x = dat[\"x\"]\n",
    "        m = dat[\"mean\"]\n",
    "        s = dat[\"std\"]\n",
    "        ax.plot(x, m, label=f\"{model_name} (AUC {dat['mean_auc']:.3f}±{dat['std_auc']:.3f})\", linewidth=2)\n",
    "        ax.fill_between(x, np.maximum(m - s, 0), np.minimum(m + s, 1), alpha=0.2)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "    ax.set_xlabel(\"False Positive Rate\", fontsize=11)\n",
    "    ax.set_ylabel(\"True Positive Rate\", fontsize=11)\n",
    "    ax.set_title(\"Mean ROC Curves (mean ± 1 sd)\", fontsize=12)\n",
    "    ax.legend(loc=\"lower right\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_mean_pr_curve(\n",
    "    fold_results_dict: Dict,\n",
    "    figsize: Tuple[int, int] = (5, 5),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制平均PR曲线\"\"\"\n",
    "    pr_data = {}\n",
    "    pos_rate = None\n",
    "    \n",
    "    for model_name, pack in fold_results_dict.items():\n",
    "        pairs = _collect_curves_from_folds(pack)\n",
    "        if not pairs:\n",
    "            continue\n",
    "        all_y = np.concatenate([y for y, _ in pairs])\n",
    "        pos_rate = float(all_y.mean()) if pos_rate is None else pos_rate\n",
    "        pr_data[model_name] = _mean_std_curve_pr(pairs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for model_name, dat in pr_data.items():\n",
    "        if dat is None:\n",
    "            continue\n",
    "        x = dat[\"x\"]\n",
    "        m = dat[\"mean\"]\n",
    "        s = dat[\"std\"]\n",
    "        ax.plot(x, m, label=f\"{model_name} (AP {dat['mean_ap']:.3f}±{dat['std_ap']:.3f})\", linewidth=2)\n",
    "        ax.fill_between(x, np.maximum(m - s, 0), np.minimum(m + s, 1), alpha=0.2)\n",
    "    \n",
    "    if pos_rate is not None:\n",
    "        ax.hlines(pos_rate, 0, 1, linestyles=\"--\", color=\"gray\", label=f\"Random (AP={pos_rate:.3f})\")\n",
    "    ax.set_xlabel(\"Recall\", fontsize=11)\n",
    "    ax.set_ylabel(\"Precision\", fontsize=11)\n",
    "    ax.set_title(\"Mean PR Curves (mean ± 1 sd)\", fontsize=12)\n",
    "    ax.legend(loc=\"lower left\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模块 - 混淆矩阵和t-SNE可视化\n",
    "def _compute_confusion_stats(fold_results_dict, target_model, normalize=True, quiet=False):\n",
    "    \"\"\"计算混淆矩阵的统计信息\"\"\"\n",
    "    if target_model not in fold_results_dict:\n",
    "        if not quiet:\n",
    "            print(f\"Model '{target_model}' not found.\")\n",
    "        return None, None\n",
    "\n",
    "    folds = fold_results_dict[target_model].get(\"fold_results\", [])\n",
    "    cm_list = []\n",
    "\n",
    "    for fr in folds:\n",
    "        y_true = fr.get(\"y_true\")\n",
    "        y_pred = fr.get(\"y_pred\") or fr.get(\"ensemble_pred\")\n",
    "        if y_true is None or y_pred is None:\n",
    "            continue\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        if normalize:\n",
    "            row_sum = cm.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum == 0] = 1\n",
    "            cm = cm.astype(\"float\") / row_sum\n",
    "        cm_list.append(cm)\n",
    "\n",
    "    if not cm_list:\n",
    "        if not quiet:\n",
    "            print(f\"No valid confusion matrices found for '{target_model}'.\")\n",
    "        return None, None\n",
    "\n",
    "    cm_array = np.stack(cm_list)\n",
    "    mean_cm = cm_array.mean(axis=0)\n",
    "    std_cm = cm_array.std(axis=0)\n",
    "    return mean_cm, std_cm\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_single(\n",
    "    fold_results_dict: Dict,\n",
    "    target_model: str = \"DL+SupCon\",\n",
    "    class_names: Tuple[str, str] = (\"Negative\", \"Positive\"),\n",
    "    normalize: bool = True,\n",
    "    figsize: Tuple[int, int] = (5, 4),\n",
    "    cmap: str = \"Blues\",\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制单个模型的平均混淆矩阵\"\"\"\n",
    "    mean_cm, std_cm = _compute_confusion_stats(fold_results_dict, target_model, normalize=normalize)\n",
    "    if mean_cm is None:\n",
    "        return None, None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(mean_cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.set_title(f\"Mean Confusion Matrix ({target_model})\", fontsize=13)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=11)\n",
    "    ax.set_ylabel(\"True label\", fontsize=11)\n",
    "\n",
    "\n",
    "    for i in range(mean_cm.shape[0]):\n",
    "        for j in range(mean_cm.shape[1]):\n",
    "            ax.text(j, i, f\"{mean_cm[i, j]:.2f}\\n±{std_cm[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"black\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n",
    "                    fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_confusion_matrices_grid(\n",
    "    fold_results_dict: Dict,\n",
    "    target_models: Optional[List[str]] = None,\n",
    "    class_names: Tuple[str, str] = (\"Negative\", \"Positive\"),\n",
    "    normalize: bool = True,\n",
    "    n_cols: int = 3,\n",
    "    figsize_per_panel: Tuple[float, float] = (4.2, 4.2),\n",
    "    cmap: str = \"Blues\",\n",
    "    save_path: Optional[str] = None,\n",
    "    save_individual_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"在同一张画布中绘制多个模型的平均混淆矩阵\"\"\"\n",
    "    if target_models is None:\n",
    "        target_models = list(fold_results_dict.keys())\n",
    "\n",
    "    valid_results = []\n",
    "    for model_name in target_models:\n",
    "        mean_cm, std_cm = _compute_confusion_stats(\n",
    "            fold_results_dict, model_name, normalize=normalize, quiet=True)\n",
    "        if mean_cm is not None:\n",
    "            valid_results.append((model_name, mean_cm, std_cm))\n",
    "        else:\n",
    "            print(f\"⚠️ 跳过模型 '{model_name}'，缺少有效的折结果。\")\n",
    "\n",
    "    if not valid_results:\n",
    "        print(\"未收集到任何有效的混淆矩阵。\")\n",
    "        return None, None\n",
    "\n",
    "    n_models = len(valid_results)\n",
    "    n_cols = max(1, min(n_cols, n_models))\n",
    "    n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "    fig_width = figsize_per_panel[0] * n_cols\n",
    "    fig_height = figsize_per_panel[1] * n_rows\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.reshape(n_rows, n_cols)\n",
    "\n",
    "    im = None\n",
    "    for idx, (model_name, mean_cm, std_cm) in enumerate(valid_results):\n",
    "        r, c = divmod(idx, n_cols)\n",
    "        ax = axes[r, c]\n",
    "        im = ax.imshow(mean_cm, interpolation=\"nearest\", cmap=cmap)\n",
    "        ax.set_title(model_name, fontsize=12)\n",
    "\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_yticks(tick_marks)\n",
    "        ax.set_xticklabels(class_names)\n",
    "        ax.set_yticklabels(class_names)\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=10)\n",
    "        ax.set_ylabel(\"True\", fontsize=10)\n",
    "\n",
    "    \n",
    "        for i in range(mean_cm.shape[0]):\n",
    "            for j in range(mean_cm.shape[1]):\n",
    "                ax.text(j, i, f\"{mean_cm[i, j]:.2f}\\n±{std_cm[i, j]:.2f}\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"black\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n",
    "                        fontsize=9)\n",
    "\n",
    "    total_slots = n_rows * n_cols\n",
    "    for idx in range(len(valid_results), total_slots):\n",
    "        r, c = divmod(idx, n_cols)\n",
    "        axes[r, c].axis(\"off\")\n",
    "\n",
    "    if im is not None:\n",
    "        cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.85)\n",
    "        cbar.set_label(\"Normalized\" if normalize else \"Count\", fontsize=10)\n",
    "\n",
    "    fig.suptitle(\"Mean Confusion Matrices\", fontsize=14, y=0.995)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "\n",
    "    # 保存单独的混淆矩阵\n",
    "    if save_individual_path:\n",
    "        os.makedirs(save_individual_path, exist_ok=True)\n",
    "        for model_name, mean_cm, std_cm in valid_results:\n",
    "            fig_individual, ax_individual = plt.subplots(figsize=(5, 4))\n",
    "            im_individual = ax_individual.imshow(mean_cm, interpolation=\"nearest\", cmap=cmap)\n",
    "\n",
    "            ax_individual.set_title(f\"{model_name} Confusion Matrix\", fontsize=12)\n",
    "\n",
    "            tick_marks = np.arange(len(class_names))\n",
    "            ax_individual.set_xticks(tick_marks)\n",
    "            ax_individual.set_yticks(tick_marks)\n",
    "            ax_individual.set_xticklabels(class_names)\n",
    "            ax_individual.set_yticklabels(class_names)\n",
    "            ax_individual.set_xlabel(\"Predicted\", fontsize=10)\n",
    "            ax_individual.set_ylabel(\"True\", fontsize=10)\n",
    "\n",
    "        \n",
    "            for i in range(mean_cm.shape[0]):\n",
    "                for j in range(mean_cm.shape[1]):\n",
    "                    ax_individual.text(j, i, f\"{mean_cm[i, j]:.2f}\\n±{std_cm[i, j]:.2f}\",\n",
    "                                       ha=\"center\", va=\"center\",\n",
    "                                       color=\"black\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n",
    "                                       fontsize=10)\n",
    "\n",
    "            cbar_individual = fig_individual.colorbar(im_individual, ax=ax_individual, shrink=0.8)\n",
    "            cbar_individual.set_label(\"Normalized\" if normalize else \"Count\", fontsize=10)\n",
    "\n",
    "            fig_individual.tight_layout()\n",
    "\n",
    "            # 生成安全的文件名\n",
    "            safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")\n",
    "            individual_save_path = os.path.join(save_individual_path, f\"confusion_matrix_{safe_model_name}.pdf\")\n",
    "            fig_individual.savefig(individual_save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "            plt.close(fig_individual)  # 关闭单独的图以释放内存\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def plot_tsne_comparison(\n",
    "    features_raw: np.ndarray,\n",
    "    features_learned: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    random_state: int = 2025,\n",
    "    figsize: Tuple[int, int] = (10, 4),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"对比原始特征和模型学习特征的t-SNE可视化\"\"\"\n",
    "    # 原始特征\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_raw)\n",
    "    tsne_raw = TSNE(n_components=2, random_state=random_state, perplexity=30)\n",
    "    raw_2d = tsne_raw.fit_transform(features_scaled)\n",
    "\n",
    "    # 学习特征\n",
    "    tsne_learned = TSNE(n_components=2, random_state=random_state, perplexity=30)\n",
    "    learned_2d = tsne_learned.fit_transform(features_learned)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # 原始特征\n",
    "    for label in np.unique(labels):\n",
    "        idx = labels == label\n",
    "        ax1.scatter(raw_2d[idx, 0], raw_2d[idx, 1], label=f\"Class {label}\", s=15, alpha=0.7)\n",
    "    ax1.set_title(\"t-SNE of Raw Gene Expression Features\", fontsize=12)\n",
    "    ax1.set_xlabel(\"Dim 1\", fontsize=11)\n",
    "    ax1.set_ylabel(\"Dim 2\", fontsize=11)\n",
    "    ax1.legend(fontsize=9)\n",
    "\n",
    "    # 学习特征\n",
    "    for label in np.unique(labels):\n",
    "        idx = labels == label\n",
    "        ax2.scatter(learned_2d[idx, 0], learned_2d[idx, 1], label=f\"Class {label}\", s=15, alpha=0.7)\n",
    "    ax2.set_title(\"t-SNE of Learned Representation (DL+SupCon)\", fontsize=12)\n",
    "    ax2.set_xlabel(\"Dim 1\", fontsize=11)\n",
    "    ax2.set_ylabel(\"Dim 2\", fontsize=11)\n",
    "    ax2.legend(fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, (ax1, ax2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7baa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可解释性分析可视化函数\n",
    "def plot_shap_summary(\n",
    "    shap_values: np.ndarray,\n",
    "    test_data: np.ndarray,\n",
    "    gene_names: Optional[List[str]] = None,\n",
    "    max_display: int = 20,\n",
    "    plot_type: str = \"summary\",\n",
    "    figsize: Tuple[int, int] = (6, 5),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制SHAP summary图\n",
    "    \n",
    "    Args:\n",
    "        shap_values: SHAP值数组 (n_samples, n_features)\n",
    "        test_data: 测试数据 (n_samples, n_features)\n",
    "        gene_names: 基因名称列表\n",
    "        max_display: 显示的最大特征数\n",
    "        plot_type: \"summary\" 或 \"bar\"\n",
    "        figsize: 图形大小\n",
    "        save_path: 保存路径\n",
    "        show: 是否显示\n",
    "    \n",
    "    Returns:\n",
    "        fig, ax: matplotlib图形对象\n",
    "    \"\"\"\n",
    "    # SHAP的summary_plot不支持ax参数，它会自己创建图形\n",
    "    # 我们需要先设置图形大小，然后让SHAP创建图形\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    if plot_type == \"summary\":\n",
    "        shap.summary_plot(\n",
    "            shap_values, test_data, \n",
    "            feature_names=gene_names,\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "    elif plot_type == \"bar\":\n",
    "        shap.summary_plot(\n",
    "            shap_values, test_data,\n",
    "            feature_names=gene_names,\n",
    "            plot_type=\"bar\",\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "    \n",
    "    # 获取SHAP创建的图形对象\n",
    "    fig = plt.gcf()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    # 注意：即使show=False，我们也保留图形对象，以便在notebook中进一步操作\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_shap_top_features(\n",
    "    shap_importance: pd.DataFrame,\n",
    "    top_n: int = 20,\n",
    "    figsize: Tuple[int, int] = (6, 5),\n",
    "    color: str = \"#4C72B0\",\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制SHAP Top N特征重要性柱状图\"\"\"\n",
    "    top_features = shap_importance.head(top_n)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.barh(range(top_n), top_features['mean_abs_shap'].values, color=color)\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=11)\n",
    "    ax.set_title(f'Top {top_n} Features by SHAP', fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_lime_top_features(\n",
    "    lime_importance: pd.DataFrame,\n",
    "    top_n: int = 20,\n",
    "    figsize: Tuple[int, int] = (6, 5),\n",
    "    color: str = \"#55A868\",\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制LIME Top N特征重要性柱状图\"\"\"\n",
    "    top_features = lime_importance.head(top_n)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.barh(range(top_n), top_features['mean_abs_importance'].values, color=color)\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Mean Absolute LIME Importance', fontsize=11)\n",
    "    ax.set_title(f'Top {top_n} Features by LIME', fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_lime_heatmap(\n",
    "    feature_importance_matrix: np.ndarray,\n",
    "    gene_names: Optional[List[str]] = None,\n",
    "    top_n_genes: int = 30,\n",
    "    n_samples: int = 50,\n",
    "    figsize: Tuple[int, int] = (8, 6),\n",
    "    cmap: str = 'RdBu_r',\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制LIME重要性热图\"\"\"\n",
    "    if feature_importance_matrix.max() == 0:\n",
    "        print(\"警告: 所有特征重要性为0，无法绘制热图\")\n",
    "        return None, None\n",
    "    \n",
    "    mean_importance = feature_importance_matrix.mean(axis=0)\n",
    "    top_features_idx = np.argsort(mean_importance)[-top_n_genes:][::-1]\n",
    "    \n",
    "    n_samples_actual = min(n_samples, feature_importance_matrix.shape[0])\n",
    "    sample_indices = np.random.choice(\n",
    "        feature_importance_matrix.shape[0], \n",
    "        n_samples_actual, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    heatmap_data = feature_importance_matrix[sample_indices][:, top_features_idx]\n",
    "    top_gene_names = [\n",
    "        gene_names[i] if gene_names else f\"Gene_{i}\" \n",
    "        for i in top_features_idx\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        heatmap_data.T,\n",
    "        yticklabels=top_gene_names,\n",
    "        xticklabels=False,\n",
    "        cmap=cmap,\n",
    "        center=0,\n",
    "        cbar_kws={'label': 'LIME Importance'},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('Samples', fontsize=11)\n",
    "    ax.set_ylabel('Genes', fontsize=11)\n",
    "    ax.set_title(f'LIME: Top {top_n_genes} Genes Importance Heatmap', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ablation_top_features(\n",
    "    ablation_importance: pd.DataFrame,\n",
    "    top_n: int = 20,\n",
    "    figsize: Tuple[int, int] = (6, 5),\n",
    "    color: str = \"coral\",\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制消融分析Top N特征重要性柱状图\"\"\"\n",
    "    top_features = ablation_importance.head(top_n)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.barh(range(top_n), top_features['mean_abs_prob_change'].values, color=color)\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Mean Absolute Probability Change', fontsize=11)\n",
    "    ax.set_title(f'Top {top_n} Features by Feature Ablation', fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ablation_distribution(\n",
    "    feature_importance_scores: np.ndarray,\n",
    "    figsize: Tuple[int, int] = (6, 4),\n",
    "    color: str = \"coral\",\n",
    "    bins: int = 50,\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制消融分析特征重要性分布图\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist(feature_importance_scores, bins=bins, alpha=0.7, color=color, edgecolor='black')\n",
    "    ax.set_xlabel('Absolute Probability Change', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title('Distribution of Feature Importance (Ablation Method)', fontsize=12)\n",
    "    ax.axvline(\n",
    "        feature_importance_scores.mean(), \n",
    "        color='red', \n",
    "        linestyle='--', \n",
    "        linewidth=2, \n",
    "        label=f'Mean: {feature_importance_scores.mean():.4f}'\n",
    "    )\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_interpretability_comparison_top_features(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 30,\n",
    "    figsize: Tuple[int, int] = (14, 5),\n",
    "    colors: Dict[str, str] = None,\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"绘制三种方法Top N特征的对比图\"\"\"\n",
    "    if colors is None:\n",
    "        colors = {\"shap\": \"#4C72B0\", \"lime\": \"#55A868\", \"ablation\": \"#C44E52\"}\n",
    "    \n",
    "    top_features = merged_df.head(top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # SHAP\n",
    "    axes[0].barh(range(top_n), top_features['shap_norm'].values, color=colors[\"shap\"])\n",
    "    axes[0].set_yticks(range(top_n))\n",
    "    axes[0].set_yticklabels(top_features['feature'].values, fontsize=8)\n",
    "    axes[0].set_xlabel('Normalized Importance', fontsize=10)\n",
    "    axes[0].set_title('SHAP', fontsize=11)\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # LIME\n",
    "    axes[1].barh(range(top_n), top_features['lime_norm'].values, color=colors[\"lime\"])\n",
    "    axes[1].set_yticks(range(top_n))\n",
    "    axes[1].set_yticklabels(top_features['feature'].values, fontsize=8)\n",
    "    axes[1].set_xlabel('Normalized Importance', fontsize=10)\n",
    "    axes[1].set_title('LIME', fontsize=11)\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Feature Ablation\n",
    "    axes[2].barh(range(top_n), top_features['ablation_norm'].values, color=colors[\"ablation\"])\n",
    "    axes[2].set_yticks(range(top_n))\n",
    "    axes[2].set_yticklabels(top_features['feature'].values, fontsize=8)\n",
    "    axes[2].set_xlabel('Normalized Importance', fontsize=10)\n",
    "    axes[2].set_title('Feature Ablation', fontsize=11)\n",
    "    axes[2].invert_yaxis()\n",
    "    axes[2].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    fig.suptitle(f'Top {top_n} Features by Different Interpretability Methods', fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def plot_interpretability_correlation(\n",
    "    merged_df: pd.DataFrame,\n",
    "    figsize: Tuple[int, int] = (14, 4),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"绘制三种方法之间的相关性散点图\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # SHAP vs LIME\n",
    "    axes[0].scatter(merged_df['shap_norm'], merged_df['lime_norm'], alpha=0.5, s=20)\n",
    "    corr_shap_lime = merged_df[['shap_norm', 'lime_norm']].corr().iloc[0, 1]\n",
    "    axes[0].set_xlabel('SHAP (normalized)', fontsize=10)\n",
    "    axes[0].set_ylabel('LIME (normalized)', fontsize=10)\n",
    "    axes[0].set_title(f'SHAP vs LIME (corr={corr_shap_lime:.3f})', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SHAP vs Ablation\n",
    "    axes[1].scatter(merged_df['shap_norm'], merged_df['ablation_norm'], alpha=0.5, s=20)\n",
    "    corr_shap_abl = merged_df[['shap_norm', 'ablation_norm']].corr().iloc[0, 1]\n",
    "    axes[1].set_xlabel('SHAP (normalized)', fontsize=10)\n",
    "    axes[1].set_ylabel('Feature Ablation (normalized)', fontsize=10)\n",
    "    axes[1].set_title(f'SHAP vs Ablation (corr={corr_shap_abl:.3f})', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # LIME vs Ablation\n",
    "    axes[2].scatter(merged_df['lime_norm'], merged_df['ablation_norm'], alpha=0.5, s=20)\n",
    "    corr_lime_abl = merged_df[['lime_norm', 'ablation_norm']].corr().iloc[0, 1]\n",
    "    axes[2].set_xlabel('LIME (normalized)', fontsize=10)\n",
    "    axes[2].set_ylabel('Feature Ablation (normalized)', fontsize=10)\n",
    "    axes[2].set_title(f'LIME vs Ablation (corr={corr_lime_abl:.3f})', fontsize=11)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def plot_interpretability_venn(\n",
    "    shap_importance: pd.DataFrame,\n",
    "    lime_importance: pd.DataFrame,\n",
    "    ablation_importance: pd.DataFrame,\n",
    "    top_n: int = 10,\n",
    "    figsize: Tuple[int, int] = (6, 6),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"绘制三种方法Top N特征的Venn图\"\"\"\n",
    "    shap_top = set(shap_importance.head(top_n)['feature'].values)\n",
    "    lime_top = set(lime_importance.head(top_n)['feature'].values)\n",
    "    abl_top = set(ablation_importance.head(top_n)['feature'].values)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    venn3(\n",
    "        [shap_top, lime_top, abl_top],\n",
    "        set_labels=('SHAP', 'LIME', 'Feature\\nAblation'),\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'Overlap of Top {top_n} Features Identified by Different Methods', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline\n",
    "def run_sklearn_baselines(features, labels, out_dir, n_folds=5, seed=2025):\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression(L2)\": Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=5))]), \n",
    "        \"LinearSVM(Calibrated)\": CalibratedClassifierCV(estimator=Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"svc\", LinearSVC(C=10, class_weight='balanced', max_iter=1000))]), method=\"sigmoid\", cv=3),\n",
    "        \"SVM-RBF\": Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"clf\", SVC(kernel=\"rbf\", C=0.5, gamma=\"scale\", probability=True))]),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=50, max_depth=None, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=seed),}\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    all_results = {}\n",
    "    for model_name, model in models.items():\n",
    "        fold_records = []\n",
    "        print(f\"\\n=== Baseline: {model_name} ===\")\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(features, labels), 1):\n",
    "            X_tr, X_te = features[tr_idx], features[te_idx]\n",
    "            y_tr, y_te = labels[tr_idx], labels[te_idx]\n",
    "\n",
    "            t0 = time.time()\n",
    "            model.fit(X_tr, y_tr)\n",
    "            train_time = time.time() - t0\n",
    "\n",
    "            # predictions and probabilities\n",
    "            y_prob = None\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_te)[:, 1]\n",
    "            elif hasattr(model, \"decision_function\"):\n",
    "                scores = model.decision_function(X_te)\n",
    "                y_prob = 1.0 / (1.0 + np.exp(-scores))\n",
    "            else:\n",
    "                y_prob = None\n",
    "\n",
    "            y_pred = model.predict(X_te)\n",
    "\n",
    "            acc = accuracy_score(y_te, y_pred)\n",
    "            f1  = f1_score(y_te, y_pred) if len(np.unique(y_te)) > 1 else float(\"nan\")\n",
    "            auc = roc_auc_score(y_te, y_prob) if (y_prob is not None and len(np.unique(y_te)) > 1) else float(\"nan\")\n",
    "\n",
    "            print(f\"Fold {fold}/{n_folds} | Acc={acc:.3f}, AUC={auc if not np.isnan(auc) else float('nan'):.3f}, F1={f1 if not np.isnan(f1) else float('nan'):.3f}\")\n",
    "\n",
    "            fold_records.append({\n",
    "                \"fold\": fold,\n",
    "                \"test_acc\": float(acc),\n",
    "                \"test_auc\": float(auc) if not np.isnan(auc) else None,\n",
    "                \"test_f1\":  float(f1)  if not np.isnan(f1)  else None,\n",
    "                \"y_true\": y_te.astype(int).tolist(),\n",
    "                \"y_prob\": y_prob.tolist() if y_prob is not None else None,\n",
    "                \"y_pred\": y_pred.astype(int).tolist()\n",
    "            })\n",
    "\n",
    "        # summary\n",
    "        accs = [r[\"test_acc\"] for r in fold_records]\n",
    "        aucs = [r[\"test_auc\"] for r in fold_records if r[\"test_auc\"] is not None]\n",
    "        f1s  = [r[\"test_f1\"]  for r in fold_records if r[\"test_f1\"]  is not None]\n",
    "\n",
    "        summary = {\n",
    "            \"mean_acc\": float(np.mean(accs)),\n",
    "            \"std_acc\":  float(np.std(accs)),\n",
    "            \"mean_auc\": float(np.mean(aucs)) if len(aucs) else None,\n",
    "            \"std_auc\":  float(np.std(aucs))  if len(aucs) else None,\n",
    "            \"mean_f1\":  float(np.mean(f1s))  if len(f1s)  else None,\n",
    "            \"std_f1\":   float(np.std(f1s))   if len(f1s)   else None,\n",
    "        }\n",
    "\n",
    "        all_results[model_name] = {\n",
    "            \"fold_results\": fold_records,\n",
    "            \"overall_metrics\": summary\n",
    "        }\n",
    "\n",
    "    # save JSON\n",
    "    out_path = os.path.join(out_dir, \"baselines_results.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\n[Baselines] Results saved to {out_path}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_results_full = train_ensemble_models(n_models=10)\n",
    "features, labels, _ = load_and_preprocess_raw_data(EXPR_FILE, LABEL_FILE, SAMPLE_ID_COL, LABEL_COL)\n",
    "baseline_results = run_sklearn_baselines(features, labels, OUT_DIR, n_folds=CONFIG[\"N_FOLDS\"], seed=CONFIG[\"RANDOM_SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50577e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary\n",
    "MAIN_JSON = os.path.join(OUT_DIR, \"ensemble_10models_results.json\")\n",
    "BASE_JSON = os.path.join(OUT_DIR, \"baselines_results.json\")\n",
    "\n",
    "def _safe_mean_std(d, mean_key, std_key):\n",
    "    m = d.get(mean_key, np.nan)\n",
    "    s = d.get(std_key,  np.nan)\n",
    "    if m is None: m = np.nan\n",
    "    if s is None: s = np.nan\n",
    "    return float(m), float(s)\n",
    "\n",
    "\n",
    "def load_models_summary(main_json=MAIN_JSON, base_json=BASE_JSON):\n",
    "    rows = []\n",
    "    if os.path.isfile(main_json):\n",
    "        with open(main_json, \"r\") as f:\n",
    "            main_sum = json.load(f)\n",
    "        om = main_sum.get(\"overall_metrics\", {})\n",
    "        acc_m, acc_s = _safe_mean_std(om, \"mean_acc\", \"std_acc\")\n",
    "        auc_m, auc_s = _safe_mean_std(om, \"mean_auc\", \"std_auc\")\n",
    "        f1_m,  f1_s  = _safe_mean_std(om, \"mean_f1\",  \"std_f1\")\n",
    "        rows.append({\n",
    "            \"model\": \"DL+SupCon\",\n",
    "            \"mean_acc\": acc_m, \"std_acc\": acc_s,\n",
    "            \"mean_auc\": auc_m, \"std_auc\": auc_s,\n",
    "            \"mean_f1\":  f1_m,  \"std_f1\":  f1_s\n",
    "        })\n",
    "\n",
    "    if os.path.isfile(base_json):\n",
    "        with open(base_json, \"r\") as f:\n",
    "            base_all = json.load(f)\n",
    "        for name, pack in base_all.items():\n",
    "            om = pack.get(\"overall_metrics\", {})\n",
    "            acc_m, acc_s = _safe_mean_std(om, \"mean_acc\", \"std_acc\")\n",
    "            auc_m, auc_s = _safe_mean_std(om, \"mean_auc\", \"std_auc\")\n",
    "            f1_m,  f1_s  = _safe_mean_std(om, \"mean_f1\",  \"std_f1\")\n",
    "            rows.append({\n",
    "                \"model\": name,\n",
    "                \"mean_acc\": acc_m, \"std_acc\": acc_s,\n",
    "                \"mean_auc\": auc_m, \"std_auc\": auc_s,\n",
    "                \"mean_f1\":  f1_m,  \"std_f1\":  f1_s\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"model\")\n",
    "    df = df.loc[sorted(df.index, key=lambda x: (0 if x == \"DL+SupCon\" else 1, x))]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_models_summary(MAIN_JSON, BASE_JSON)\n",
    "table_csv = os.path.join(OUT_DIR, \"metrics_summary_table.csv\")\n",
    "df.to_csv(table_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68da925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fold_results_dict = {}\n",
    "with open(MAIN_JSON) as f:\n",
    "    main_res = json.load(f)\n",
    "fold_results_dict[\"DL+SupCon\"] = main_res\n",
    "\n",
    "with open(BASE_JSON) as f:\n",
    "    base_all = json.load(f)\n",
    "for name, pack in base_all.items():\n",
    "    fold_results_dict[name] = pack\n",
    "\n",
    "plot_three_metrics_separate_subplots(df, fold_results_dict, save_path=\"/home/fujing/ad_ssl/3_ssl_model/metrics_comparison.pdf\")\n",
    "plot_mean_roc_curve(fold_results_dict, save_path=\"/home/fujing/ad_ssl/3_ssl_model/mean_roc_curve.pdf\")\n",
    "plot_mean_pr_curve(fold_results_dict, save_path=\"/home/fujing/ad_ssl/3_ssl_model/mean_pr_curve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_plot = [\n",
    "    \"DL+SupCon\",\n",
    "    \"RandomForest\",\n",
    "    \"LogisticRegression(L2)\",\n",
    "    \"LinearSVM(Calibrated)\",\n",
    "    \"SVM-RBF\"\n",
    "]\n",
    "\n",
    "plot_confusion_matrices_grid(\n",
    "    fold_results_dict,\n",
    "    target_models=models_to_plot,\n",
    "    class_names=(\"Control\", \"AD\"),\n",
    "    normalize=True,\n",
    "    n_cols=3,\n",
    "    save_path=\"/home/fujing/ad_ssl/3_ssl_model/confusion_matrix_all_models.pdf\",\n",
    "    save_individual_path=\"/home/fujing/ad_ssl/3_ssl_model/individual_confusion_matrices\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最后一折模型进行 t-SNE 可视化\n",
    "device = get_device()\n",
    "input_dim = features.shape[1]\n",
    "features_raw = features \n",
    "\n",
    "# 重新创建模型结构并加载权重\n",
    "model_path = os.path.join(OUT_DIR, \"ensemble_fold3_model7.pth\")  # 也可以换成表现最好的一折\n",
    "model = GeneClassifier(input_dim=input_dim,\n",
    "                       hidden_dims=CONFIG[\"HIDDEN_DIMS\"],\n",
    "                       num_classes=2,\n",
    "                       dropout_rate=CONFIG[\"DROPOUT\"],\n",
    "                       use_batch_norm=CONFIG[\"USE_BATCH_NORM\"],\n",
    "                       proj_dim=CONFIG[\"PROJ_DIM\"]).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# 创建完整数据的 DataLoader 用于提取特征\n",
    "dataset_all = GeneDataset(features, labels)\n",
    "loader_all = DataLoader(dataset_all, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f01bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tsne_comparison\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            # 使用模型的feature_extractor提取中间层特征\n",
    "            feats = model.feature_extractor(x)\n",
    "            all_feats.append(feats.cpu().numpy())\n",
    "            all_labels.append(y.numpy())\n",
    "    feats = np.concatenate(all_feats, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    return feats, labels\n",
    "\n",
    "# 提取学习到的特征\n",
    "features_learned, labels_extracted = extract_features(model, loader_all, device)\n",
    "\n",
    "# 可视化\n",
    "fig, (ax1, ax2) = plot_tsne_comparison(\n",
    "    features_raw=features_raw,           # 原始特征矩阵 (n_samples, n_features)\n",
    "    features_learned=features_learned,   # 模型学习到的特征 (n_samples, hidden_dim)\n",
    "    labels=labels_extracted,            # 标签数组 (n_samples,)\n",
    "    random_state=2025,                   # 随机种子，保证结果可复现\n",
    "    figsize=(8, 4),                     # 图形大小 (宽度, 高度)\n",
    "    save_path=os.path.join(OUT_DIR, \"tsne_comparison.pdf\"),  # 保存路径（可选）\n",
    "    show=True                            # 是否在notebook中显示\n",
    ")\n",
    "\n",
    "ax1.set_title(\"t-SNE of Raw Gene Expression Features\", fontsize=14)\n",
    "ax2.set_title(\"t-SNE of Learned Representation (DL+SupCon)\", fontsize=14)\n",
    "ax1.tick_params(labelsize=11)\n",
    "ax2.tick_params(labelsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddeb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外的可解释性联合分析可视化函数\n",
    "\n",
    "def plot_interpretability_heatmap(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 30,\n",
    "    figsize: Tuple[int, int] = (10, 12),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制Top N基因在三种方法中的重要性热图\n",
    "    \n",
    "    显示每个基因在SHAP、LIME、Ablation中的标准化重要性分数\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 准备热图数据\n",
    "    heatmap_data = top_features[['shap_norm', 'lime_norm', 'ablation_norm']].T\n",
    "    heatmap_data.columns = top_features['feature'].values\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Normalized Importance'},\n",
    "        yticklabels=['SHAP', 'LIME', 'Ablation'],\n",
    "        xticklabels=heatmap_data.columns,\n",
    "        ax=ax,\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    ax.set_title(f'Feature Importance Heatmap (Top {top_n} Genes)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Genes', fontsize=11)\n",
    "    ax.set_ylabel('Methods', fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_interpretability_ranking_comparison(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 30,\n",
    "    figsize: Tuple[int, int] = (12, 8),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制特征重要性排名对比图\n",
    "    \n",
    "    显示每个基因在不同方法中的排名位置\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 计算排名\n",
    "    top_features['shap_rank'] = range(1, len(top_features) + 1)\n",
    "    top_features['lime_rank'] = top_features['lime_norm'].rank(ascending=False, method='min').astype(int)\n",
    "    top_features['ablation_rank'] = top_features['ablation_norm'].rank(ascending=False, method='min').astype(int)\n",
    "    \n",
    "    # 准备数据\n",
    "    genes = top_features['feature'].values\n",
    "    x = np.arange(len(genes))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    ax.bar(x - width, top_features['shap_rank'], width, label='SHAP', color='#4C72B0', alpha=0.8)\n",
    "    ax.bar(x, top_features['lime_rank'], width, label='LIME', color='#55A868', alpha=0.8)\n",
    "    ax.bar(x + width, top_features['ablation_rank'], width, label='Ablation', color='#C44E52', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Genes', fontsize=11)\n",
    "    ax.set_ylabel('Rank', fontsize=11)\n",
    "    ax.set_title(f'Feature Importance Ranking Comparison (Top {top_n} Genes)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(genes, rotation=45, ha='right', fontsize=8)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.invert_yaxis()  # 排名越小越好，所以反转y轴\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_consensus_score_heatmap(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 50,\n",
    "    figsize: Tuple[int, int] = (12, 8),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制一致性得分热图\n",
    "    \n",
    "    计算每个基因在三种方法中的一致性得分（有多少种方法认为它重要）\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 计算一致性得分：如果标准化重要性 > 阈值，则认为该方法认为该基因重要\n",
    "    threshold = 0.3  # 可以调整\n",
    "    top_features['shap_important'] = (top_features['shap_norm'] > threshold).astype(int)\n",
    "    top_features['lime_important'] = (top_features['lime_norm'] > threshold).astype(int)\n",
    "    top_features['ablation_important'] = (top_features['ablation_norm'] > threshold).astype(int)\n",
    "    \n",
    "    # 一致性得分 = 三种方法中认为重要的数量\n",
    "    top_features['consensus_score'] = (\n",
    "        top_features['shap_important'] + \n",
    "        top_features['lime_important'] + \n",
    "        top_features['ablation_important']\n",
    "    )\n",
    "    \n",
    "    # 准备热图数据\n",
    "    heatmap_data = top_features[['shap_important', 'lime_important', 'ablation_important', 'consensus_score']].T\n",
    "    heatmap_data.columns = top_features['feature'].values\n",
    "    \n",
    "    # 按一致性得分排序（使用列名而不是位置索引）\n",
    "    sorted_features = top_features.sort_values('consensus_score', ascending=False)['feature'].values\n",
    "    heatmap_data = heatmap_data[sorted_features]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='RdYlGn',\n",
    "        cbar_kws={'label': 'Important (1) or Not (0)'},\n",
    "        yticklabels=['SHAP', 'LIME', 'Ablation', 'Consensus'],\n",
    "        xticklabels=heatmap_data.columns,\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        vmin=0,\n",
    "        vmax=3\n",
    "    )\n",
    "    ax.set_title(f'Consensus Score Heatmap (Top {top_n} Genes, Threshold={threshold})', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Genes', fontsize=11)\n",
    "    ax.set_ylabel('Methods', fontsize=11)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_cumulative_importance(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 50,\n",
    "    figsize: Tuple[int, int] = (10, 6),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制累积重要性图\n",
    "    \n",
    "    显示Top N基因累积解释了多少预测重要性\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 计算累积重要性（归一化到0-1）\n",
    "    cumulative_shap = top_features['shap_norm'].cumsum() / top_features['shap_norm'].sum()\n",
    "    cumulative_lime = top_features['lime_norm'].cumsum() / top_features['lime_norm'].sum()\n",
    "    cumulative_ablation = top_features['ablation_norm'].cumsum() / top_features['ablation_norm'].sum()\n",
    "    cumulative_avg = top_features['avg_importance'].cumsum() / top_features['avg_importance'].sum()\n",
    "    \n",
    "    x = np.arange(1, len(top_features) + 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(x, cumulative_shap, label='SHAP', linewidth=2, marker='o', markersize=4)\n",
    "    ax.plot(x, cumulative_lime, label='LIME', linewidth=2, marker='s', markersize=4)\n",
    "    ax.plot(x, cumulative_ablation, label='Ablation', linewidth=2, marker='^', markersize=4)\n",
    "    ax.plot(x, cumulative_avg, label='Average', linewidth=2, linestyle='--', color='black')\n",
    "    \n",
    "    # 添加80%和90%的参考线\n",
    "    ax.axhline(y=0.8, color='gray', linestyle=':', alpha=0.7, label='80%')\n",
    "    ax.axhline(y=0.9, color='gray', linestyle=':', alpha=0.7, label='90%')\n",
    "    \n",
    "    ax.set_xlabel('Number of Top Features', fontsize=11)\n",
    "    ax.set_ylabel('Cumulative Normalized Importance', fontsize=11)\n",
    "    ax.set_title(f'Cumulative Importance of Top {top_n} Features', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_3d_importance_scatter(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 50,\n",
    "    figsize: Tuple[int, int] = (12, 10),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制3D散点图\n",
    "    \n",
    "    在三维空间中展示三种方法的重要性分数\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # 散点图\n",
    "    scatter = ax.scatter(\n",
    "        top_features['shap_norm'],\n",
    "        top_features['lime_norm'],\n",
    "        top_features['ablation_norm'],\n",
    "        c=top_features['avg_importance'],\n",
    "        s=50,\n",
    "        alpha=0.6,\n",
    "        cmap='viridis',\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    \n",
    "    # 标注Top 10基因\n",
    "    top10 = top_features.head(10)\n",
    "    for idx, row in top10.iterrows():\n",
    "        ax.text(\n",
    "            row['shap_norm'],\n",
    "            row['lime_norm'],\n",
    "            row['ablation_norm'],\n",
    "            row['feature'],\n",
    "            fontsize=7,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('SHAP (normalized)', fontsize=10)\n",
    "    ax.set_ylabel('LIME (normalized)', fontsize=10)\n",
    "    ax.set_zlabel('Ablation (normalized)', fontsize=10)\n",
    "    ax.set_title(f'3D Feature Importance Space (Top {top_n} Genes)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # 颜色条\n",
    "    cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('Average Importance', fontsize=10)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_hierarchical_clustering_heatmap(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 50,\n",
    "    figsize: Tuple[int, int] = (14, 10),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制层次聚类热图\n",
    "    \n",
    "    对特征进行聚类，显示相似的重要性模式\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 准备数据\n",
    "    data_for_clustering = top_features[['shap_norm', 'lime_norm', 'ablation_norm']].T\n",
    "    data_for_clustering.columns = top_features['feature'].values\n",
    "    \n",
    "    # 绘制聚类热图\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.clustermap(\n",
    "        data_for_clustering,\n",
    "        method='ward',\n",
    "        metric='euclidean',\n",
    "        cmap='YlOrRd',\n",
    "        figsize=figsize,\n",
    "        cbar_kws={'label': 'Normalized Importance'},\n",
    "        row_cluster=True,\n",
    "        col_cluster=True,\n",
    "        annot=False,\n",
    "        fmt='.2f',\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    \n",
    "    plt.suptitle(f'Hierarchical Clustering Heatmap (Top {top_n} Genes)', \n",
    "                fontsize=13, fontweight='bold', y=0.98)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_consensus_distribution(\n",
    "    merged_df: pd.DataFrame,\n",
    "    top_n: int = 100,\n",
    "    figsize: Tuple[int, int] = (10, 6),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    绘制一致性得分分布图\n",
    "    \n",
    "    显示有多少基因被1种、2种、3种方法同时识别为重要\n",
    "    \"\"\"\n",
    "    top_features = merged_df.head(top_n).copy()\n",
    "    \n",
    "    # 计算一致性得分\n",
    "    threshold = 0.3\n",
    "    top_features['consensus_count'] = (\n",
    "        (top_features['shap_norm'] > threshold).astype(int) +\n",
    "        (top_features['lime_norm'] > threshold).astype(int) +\n",
    "        (top_features['ablation_norm'] > threshold).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 统计分布\n",
    "    consensus_dist = top_features['consensus_count'].value_counts().sort_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # 柱状图\n",
    "    colors = ['#C44E52', '#F39C12', '#55A868']\n",
    "    bars = axes[0].bar(consensus_dist.index, consensus_dist.values, \n",
    "                      color=[colors[i-1] if i <= 3 else 'gray' for i in consensus_dist.index],\n",
    "                      alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Number of Methods Agreeing', fontsize=11)\n",
    "    axes[0].set_ylabel('Number of Genes', fontsize=11)\n",
    "    axes[0].set_title('Consensus Distribution', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 饼图\n",
    "    labels = [f'{i} method(s)' for i in consensus_dist.index]\n",
    "    axes[1].pie(consensus_dist.values, labels=labels, autopct='%1.1f%%',\n",
    "               colors=[colors[i-1] if i <= 3 else 'gray' for i in consensus_dist.index],\n",
    "               startangle=90)\n",
    "    axes[1].set_title('Consensus Distribution (Pie Chart)', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'Consensus Score Distribution (Top {top_n} Genes)', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46015458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可解释性分析\n",
    "# 1. SHAP Analysis using DeepExplainer\n",
    "def shap_analysis_deep(model, dataloader, device, \n",
    "                       num_background=50, num_test=100,\n",
    "                       save_dir=\"/home/fujing/ad_ssl/3_ssl_model/shap\",\n",
    "                       gene_names=None,\n",
    "                       figsize=(6, 5),\n",
    "                       save_plots=True,\n",
    "                       show_plots=True):\n",
    "    \"\"\"\n",
    "    SHAP分析函数\n",
    "    \n",
    "    Returns:\n",
    "        shap_pos: SHAP值数组\n",
    "        importance: 特征重要性DataFrame\n",
    "        figs: 可视化图形字典 {'beeswarm': fig1, 'bar': fig2, 'top_features': fig3}\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    all_data = []\n",
    "    for x, y in dataloader:\n",
    "        all_data.append(x.numpy())\n",
    "        if len(all_data) * x.shape[0] >= num_background + num_test:\n",
    "            break\n",
    "    \n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    bg = torch.tensor(all_data[:num_background], dtype=torch.float32).to(device)\n",
    "    test = torch.tensor(all_data[num_background:num_background+num_test], dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(f\"Background: {bg.shape}, Test: {test.shape}\")\n",
    "    \n",
    "    # SHAP分析\n",
    "    explainer = shap.DeepExplainer(model, bg)\n",
    "    print(\"Computing SHAP values...\")\n",
    "    shap_values = explainer.shap_values(test)\n",
    "    \n",
    "    # 处理维度\n",
    "    shap_pos = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "    print(f\"Original SHAP shape: {shap_pos.shape}\")\n",
    "    \n",
    "    # ★ 关键修复：如果是3维，只取AD类（类别1）\n",
    "    if shap_pos.ndim == 3:\n",
    "        shap_pos = shap_pos[:, :, 1]  # 取第二个类别（AD）\n",
    "        print(f\"Fixed SHAP shape: {shap_pos.shape}\")  # 现在是 (100, 421)\n",
    "    \n",
    "    test_np = test.cpu().numpy()\n",
    "    \n",
    "    # 计算特征重要性 - 现在是一维了\n",
    "    mean_shap = np.abs(shap_pos).mean(axis=0)  # (421,)\n",
    "    \n",
    "    # 创建DataFrame - 不会报错了\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': gene_names if gene_names else [f\"Gene_{i}\" for i in range(len(mean_shap))],\n",
    "        'mean_abs_shap': mean_shap\n",
    "    }).sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    importance.to_csv(os.path.join(save_dir, \"shap_importance_fixed.csv\"), index=False)\n",
    "    \n",
    "    # 可视化 - 使用可视化函数\n",
    "    figs = {}\n",
    "    \n",
    "    # 1. SHAP summary plot (beeswarm)\n",
    "    fig1, ax1 = plot_shap_summary(\n",
    "        shap_pos, test_np, gene_names=gene_names,\n",
    "        max_display=20, plot_type=\"summary\",\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"shap_beeswarm_fixed.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['beeswarm'] = (fig1, ax1)\n",
    "    \n",
    "    # 2. SHAP bar plot\n",
    "    fig2, ax2 = plot_shap_summary(\n",
    "        shap_pos, test_np, gene_names=gene_names,\n",
    "        max_display=20, plot_type=\"bar\",\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"shap_bar_fixed.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['bar'] = (fig2, ax2)\n",
    "    \n",
    "    # 3. Top features bar plot\n",
    "    fig3, ax3 = plot_shap_top_features(\n",
    "        importance, top_n=20,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"shap_top20_features.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['top_features'] = (fig3, ax3)\n",
    "    \n",
    "    return shap_pos, importance, figs\n",
    "\n",
    "# 2. LIME Analysis\n",
    "def lime_analysis(model, dataloader, device,\n",
    "                  num_samples=200,\n",
    "                  num_features=30,\n",
    "                  num_perturbations=3000,\n",
    "                  save_dir=\"/home/fujing/ad_ssl/3_ssl_model/lime\",\n",
    "                  gene_names=None,\n",
    "                  figsize=(6, 5),\n",
    "                  save_plots=True,\n",
    "                  show_plots=True):\n",
    "    \"\"\"\n",
    "    LIME分析函数\n",
    "    \n",
    "    Returns:\n",
    "        feature_importance_matrix: 特征重要性矩阵\n",
    "        feature_importance: 特征重要性DataFrame\n",
    "        figs: 可视化图形字典 {'top_features': fig1, 'heatmap': fig2}\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    # 收集数据\n",
    "    print(\"\\n[1/4] 收集数据...\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for x, y in dataloader:\n",
    "        all_data.append(x.numpy())\n",
    "        all_labels.append(y.numpy())\n",
    "        if len(all_data) * x.shape[0] >= num_samples * 2:\n",
    "            break\n",
    "    \n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    train_data = all_data[:num_samples]\n",
    "    test_data = all_data[num_samples:num_samples*2][:num_samples]\n",
    "    test_labels = all_labels[num_samples:num_samples*2][:num_samples]\n",
    "    \n",
    "    print(f\"  ✓ 训练数据: {train_data.shape}\")\n",
    "    print(f\"  ✓ 测试数据: {test_data.shape}\")\n",
    "    \n",
    "    # 定义预测函数\n",
    "    def predict_fn(X):\n",
    "        \"\"\"LIME需要的预测函数\"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "    \n",
    "    # 创建LIME explainer\n",
    "    print(\"\\n[2/4] 创建 LIME Explainer...\")\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=train_data,\n",
    "        feature_names=gene_names if gene_names else [f\"Gene_{i}\" for i in range(train_data.shape[1])],\n",
    "        class_names=['Control', 'AD'],\n",
    "        mode='classification',\n",
    "        random_state=2025\n",
    "    )\n",
    "    \n",
    "    # 测试单个样本看看LIME返回什么\n",
    "    print(\"\\n[DEBUG] 测试LIME输出格式...\")\n",
    "    test_exp = explainer.explain_instance(\n",
    "        test_data[0],\n",
    "        predict_fn,\n",
    "        num_features=10,\n",
    "        num_samples=500\n",
    "    )\n",
    "    \n",
    "    print(f\"  as_list() 示例: {test_exp.as_list()[:3]}\")\n",
    "    print(f\"  as_map() 示例: {list(test_exp.as_map()[1][:3])}\")  # 类别1的特征\n",
    "    \n",
    "    # 对每个样本进行解释\n",
    "    print(f\"\\n[3/4] 分析 {num_samples} 个样本...\")\n",
    "    all_explanations = []\n",
    "    feature_importance_matrix = np.zeros((num_samples, train_data.shape[1]))\n",
    "    \n",
    "    for idx in tqdm(range(num_samples), desc=\"LIME分析进度\"):\n",
    "        explanation = explainer.explain_instance(\n",
    "            test_data[idx],\n",
    "            predict_fn,\n",
    "            num_features=num_features,\n",
    "            num_samples=num_perturbations\n",
    "        )\n",
    "        all_explanations.append(explanation)\n",
    "        \n",
    "        # ★★★ 修复：使用as_map()而不是as_list() ★★★\n",
    "        # as_map()返回 {类别: [(特征索引, 重要性), ...]}\n",
    "        exp_map = explanation.as_map()\n",
    "        \n",
    "        # 获取AD类（类别1）的解释\n",
    "        if 1 in exp_map:\n",
    "            for feat_idx, importance in exp_map[1]:\n",
    "                feature_importance_matrix[idx, feat_idx] = abs(importance)\n",
    "        \n",
    "        # 如果类别1没有，尝试获取所有类别\n",
    "        elif 0 in exp_map:\n",
    "            for feat_idx, importance in exp_map[0]:\n",
    "                feature_importance_matrix[idx, feat_idx] = abs(importance)\n",
    "    \n",
    "    print(\"LIME分析完成\")\n",
    "    \n",
    "    # 计算平均特征重要性\n",
    "    print(\"\\n[4/4] 计算特征重要性...\")\n",
    "    mean_importance = feature_importance_matrix.mean(axis=0)\n",
    "    \n",
    "    # 调试信息\n",
    "    print(f\"  非零特征数: {np.sum(mean_importance > 0)}\")\n",
    "    print(f\"  最大重要性: {mean_importance.max():.6f}\")\n",
    "    print(f\"  平均重要性: {mean_importance.mean():.6f}\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': gene_names if gene_names else [f\"Gene_{i}\" for i in range(len(mean_importance))],\n",
    "        'mean_abs_importance': mean_importance\n",
    "    }).sort_values('mean_abs_importance', ascending=False)\n",
    "    \n",
    "    # 保存结果\n",
    "    feature_importance.to_csv(os.path.join(save_dir, \"lime_feature_importance_fixed.csv\"), index=False)\n",
    "    \n",
    "    # 可视化 - 使用可视化函数\n",
    "    figs = {}\n",
    "    \n",
    "    # 1. Top features bar plot\n",
    "    fig1, ax1 = plot_lime_top_features(\n",
    "        feature_importance, top_n=20,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"lime_top20_features_fixed.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['top_features'] = (fig1, ax1)\n",
    "    \n",
    "    # 2. Heatmap\n",
    "    if mean_importance.max() > 0:  # 只在有非零值时生成热图\n",
    "        fig2, ax2 = plot_lime_heatmap(\n",
    "            feature_importance_matrix,\n",
    "            gene_names=gene_names,\n",
    "            top_n_genes=30,\n",
    "            n_samples=50,\n",
    "            figsize=figsize,\n",
    "            save_path=os.path.join(save_dir, \"lime_importance_heatmap_fixed.pdf\") if save_plots else None,\n",
    "            show=show_plots\n",
    "        )\n",
    "        if fig2 is not None:\n",
    "            figs['heatmap'] = (fig2, ax2)\n",
    "            print(f\"LIME importance heatmap saved\")\n",
    "    \n",
    "    print(\"LIME分析完成!\")\n",
    "    return feature_importance_matrix, feature_importance, figs\n",
    "\n",
    "# 3. Feature Ablation Analysis\n",
    "def feature_ablation_analysis(model, dataloader, device,\n",
    "                              num_samples=200,\n",
    "                              ablation_baseline='mean',\n",
    "                              save_dir=\"/home/fujing/ad_ssl/3_ssl_model/ablation\",\n",
    "                              gene_names=None,\n",
    "                              figsize=(6, 5),\n",
    "                              save_plots=True,\n",
    "                              show_plots=True):\n",
    "    \"\"\"\n",
    "    Feature Ablation分析函数\n",
    "    \n",
    "    Returns:\n",
    "        feature_importance_scores: 特征重要性分数数组\n",
    "        feature_importance: 特征重要性DataFrame\n",
    "        figs: 可视化图形字典 {'top_features': fig1, 'distribution': fig2}\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 收集数据\n",
    "    print(\"\\n[1/4] 收集数据...\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for x, y in dataloader:\n",
    "        all_data.append(x.numpy())\n",
    "        all_labels.append(y.numpy())\n",
    "        if len(all_data) * x.shape[0] >= num_samples:\n",
    "            break\n",
    "    \n",
    "    all_data = np.concatenate(all_data, axis=0)[:num_samples]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)[:num_samples]\n",
    "    \n",
    "    n_features = all_data.shape[1]\n",
    "    print(f\"  ✓ 数据形状: {all_data.shape}\")\n",
    "    print(f\"  ✓ 特征数量: {n_features}\")\n",
    "    \n",
    "    # 计算baseline值\n",
    "    print(f\"\\n[2/4] 计算baseline值 (方法: {ablation_baseline})...\")\n",
    "    if ablation_baseline == 'zero':\n",
    "        baseline_values = np.zeros(n_features)\n",
    "    elif ablation_baseline == 'mean':\n",
    "        baseline_values = all_data.mean(axis=0)\n",
    "    elif ablation_baseline == 'median':\n",
    "        baseline_values = np.median(all_data, axis=0)\n",
    "    else:\n",
    "        baseline_values = np.zeros(n_features)\n",
    "    \n",
    "    # 获取原始预测\n",
    "    print(\"\\n[3/4] 计算原始预测...\")\n",
    "    original_data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        original_outputs = model(original_data_tensor)\n",
    "        original_probs = torch.softmax(original_outputs, dim=1)[:, 1].cpu().numpy()  # AD类的概率\n",
    "    \n",
    "    print(f\"  ✓ 原始预测完成，平均AD概率: {original_probs.mean():.4f}\")\n",
    "    \n",
    "    # 对每个特征进行消除分析\n",
    "    print(f\"\\n[4/4] 逐个消除特征并观察影响 ({n_features} 个特征)...\")\n",
    "    feature_importance_scores = np.zeros(n_features)\n",
    "    \n",
    "    for feat_idx in tqdm(range(n_features), desc=\"Feature Ablation进度\"):\n",
    "        # 创建消除该特征的数据\n",
    "        ablated_data = all_data.copy()\n",
    "        ablated_data[:, feat_idx] = baseline_values[feat_idx]\n",
    "        \n",
    "        # 预测\n",
    "        ablated_tensor = torch.tensor(ablated_data, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            ablated_outputs = model(ablated_tensor)\n",
    "            ablated_probs = torch.softmax(ablated_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        \n",
    "        # 计算预测差异（原始 - 消除后）\n",
    "        prob_diff = np.abs(original_probs - ablated_probs)\n",
    "        feature_importance_scores[feat_idx] = prob_diff.mean()\n",
    "    \n",
    "    print(\"  ✓ Feature Ablation分析完成\")\n",
    "    \n",
    "    # 创建特征重要性DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': gene_names if gene_names else [f\"Gene_{i}\" for i in range(n_features)],\n",
    "        'mean_abs_prob_change': feature_importance_scores\n",
    "    }).sort_values('mean_abs_prob_change', ascending=False)\n",
    "    \n",
    "    # 保存结果\n",
    "    feature_importance.to_csv(os.path.join(save_dir, \"ablation_feature_importance.csv\"), index=False)\n",
    "    \n",
    "    # 可视化 - 使用可视化函数\n",
    "    figs = {}\n",
    "    \n",
    "    # 1. Top features bar plot\n",
    "    fig1, ax1 = plot_ablation_top_features(\n",
    "        feature_importance, top_n=20,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"ablation_top20_features.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['top_features'] = (fig1, ax1)\n",
    "    \n",
    "    # 2. Distribution plot\n",
    "    fig2, ax2 = plot_ablation_distribution(\n",
    "        feature_importance_scores,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"ablation_importance_distribution.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['distribution'] = (fig2, ax2)\n",
    "    \n",
    "    return feature_importance_scores, feature_importance, figs\n",
    "\n",
    "# 4. 对比三种方法的结果\n",
    "def compare_interpretability_methods(shap_importance, lime_importance, ablation_importance,\n",
    "                                     save_dir=\"/home/fujing/ad_ssl/3_ssl_model/comparison\",\n",
    "                                     top_n=10,\n",
    "                                     figsize=(12, 4),\n",
    "                                     save_plots=True,\n",
    "                                     show_plots=True):\n",
    "    \"\"\"\n",
    "    对比三种可解释性方法的结果\n",
    "    \n",
    "    Returns:\n",
    "        merged: 合并后的特征重要性DataFrame\n",
    "        figs: 可视化图形字典 {'top_features': fig1, 'correlation': fig2, 'venn': fig3}\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    # 标准化重要性分数\n",
    "    shap_df = shap_importance.copy()\n",
    "    shap_df['shap_norm'] = (shap_df['mean_abs_shap'] - shap_df['mean_abs_shap'].min()) / \\\n",
    "                           (shap_df['mean_abs_shap'].max() - shap_df['mean_abs_shap'].min())\n",
    "    \n",
    "    lime_df = lime_importance.copy()\n",
    "    lime_df['lime_norm'] = (lime_df['mean_abs_importance'] - lime_df['mean_abs_importance'].min()) / \\\n",
    "                           (lime_df['mean_abs_importance'].max() - lime_df['mean_abs_importance'].min())\n",
    "    \n",
    "    abl_df = ablation_importance.copy()\n",
    "    abl_df['ablation_norm'] = (abl_df['mean_abs_prob_change'] - abl_df['mean_abs_prob_change'].min()) / \\\n",
    "                              (abl_df['mean_abs_prob_change'].max() - abl_df['mean_abs_prob_change'].min())\n",
    "    \n",
    "    # 合并三种方法的结果\n",
    "    merged = shap_df[['feature', 'shap_norm']].merge(\n",
    "        lime_df[['feature', 'lime_norm']], on='feature', how='outer'\n",
    "    ).merge(\n",
    "        abl_df[['feature', 'ablation_norm']], on='feature', how='outer'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # 计算平均重要性\n",
    "    merged['avg_importance'] = (merged['shap_norm'] + merged['lime_norm'] + merged['ablation_norm']) / 3\n",
    "    merged = merged.sort_values('avg_importance', ascending=False)\n",
    "    \n",
    "    # 保存结果\n",
    "    merged.to_csv(os.path.join(save_dir, \"combined_feature_importance.csv\"), index=False)\n",
    "    \n",
    "    # 可视化 - 使用可视化函数\n",
    "    figs = {}\n",
    "    \n",
    "    # 1. Top N特征的比较\n",
    "    fig1, axes1 = plot_interpretability_comparison_top_features(\n",
    "        merged, top_n=top_n,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, f\"comparison_top{top_n}_features.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['top_features'] = (fig1, axes1)\n",
    "    \n",
    "    # 2. 相关性分析\n",
    "    fig2, axes2 = plot_interpretability_correlation(\n",
    "        merged,\n",
    "        figsize=figsize,\n",
    "        save_path=os.path.join(save_dir, \"method_correlation.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['correlation'] = (fig2, axes2)\n",
    "    \n",
    "    # 3. Venn diagram\n",
    "    fig3, ax3 = plot_interpretability_venn(\n",
    "        shap_df, lime_df, abl_df,\n",
    "        top_n=20,\n",
    "        figsize=(6, 6),\n",
    "        save_path=os.path.join(save_dir, \"venn_diagram_top20.pdf\") if save_plots else None,\n",
    "        show=show_plots\n",
    "    )\n",
    "    figs['venn'] = (fig3, ax3)\n",
    "    \n",
    "    return merged, figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载数据\n",
    "features, labels, gene_names = load_and_preprocess_raw_data(EXPR_FILE, LABEL_FILE, \"id\", \"group\", CONFIG)\n",
    "\n",
    "# 2. 加载模型\n",
    "MODEL_PATH = \"/home/fujing/ad_ssl/3_ssl_model/ensemble_fold3_model7.pth\"\n",
    "device = get_device()\n",
    "model = GeneClassifier(\n",
    "    input_dim=features.shape[1],\n",
    "    hidden_dims=CONFIG[\"HIDDEN_DIMS\"],\n",
    "    num_classes=2,\n",
    "    dropout_rate=CONFIG[\"DROPOUT\"],\n",
    "    use_batch_norm=CONFIG[\"USE_BATCH_NORM\"],\n",
    "    proj_dim=CONFIG[\"PROJ_DIM\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 3. 创建DataLoader\n",
    "dataset_all = GeneDataset(features, labels)\n",
    "loader_all = DataLoader(dataset_all, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAP分析\n",
    "shap_values, shap_importance, shap_figs = shap_analysis_deep(\n",
    "    model=model,\n",
    "    dataloader=loader_all,\n",
    "    device=device,\n",
    "    num_background=250,\n",
    "    num_test=100,\n",
    "    save_dir=os.path.join(OUT_DIR, \"shap\"),\n",
    "    gene_names=gene_names,  # 传递基因名称\n",
    "    figsize=(6, 5),\n",
    "    save_plots=True,\n",
    "    show_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LIME分析\n",
    "lime_matrix, lime_importance, lime_figs = lime_analysis(\n",
    "    model=model,\n",
    "    dataloader=loader_all,\n",
    "    device=device,\n",
    "    num_samples=200,  # LIME比较慢，用100个样本\n",
    "    num_features=30,\n",
    "    num_perturbations=3000,\n",
    "    save_dir=os.path.join(OUT_DIR, \"lime\"),\n",
    "    gene_names=gene_names,  # 传递基因名称\n",
    "    figsize=(6, 5),\n",
    "    save_plots=True,\n",
    "    show_plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature Ablation分析\n",
    "ablation_scores, ablation_importance, ablation_figs = feature_ablation_analysis(\n",
    "    model=model,\n",
    "    dataloader=loader_all,\n",
    "    device=device,\n",
    "    num_samples=500,\n",
    "    ablation_baseline='mean',\n",
    "    save_dir=os.path.join(OUT_DIR, \"ablation\"),\n",
    "    gene_names=gene_names,  # 传递基因名称\n",
    "    figsize=(6, 5),\n",
    "    save_plots=True,\n",
    "    show_plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 对比三种方法\n",
    "combined_importance, comparison_figs = compare_interpretability_methods(\n",
    "    shap_importance=shap_importance,\n",
    "    lime_importance=lime_importance,\n",
    "    ablation_importance=ablation_importance,\n",
    "    save_dir=os.path.join(OUT_DIR, \"comparison_v2\"),\n",
    "    top_n=10,\n",
    "    figsize=(12, 4),\n",
    "    save_plots=True,\n",
    "    show_plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee391658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成额外的联合分析可视化图\n",
    "\n",
    "# 确保combined_importance已经生成\n",
    "if 'combined_importance' in locals():\n",
    "    print(\"生成额外的联合分析可视化图...\\n\")\n",
    "    \n",
    "    save_dir = os.path.join(OUT_DIR, \"comparison_v2\")\n",
    "    \n",
    "    # 1. 重要性热图\n",
    "    print(\"1. 生成重要性热图...\")\n",
    "    fig1, ax1 = plot_interpretability_heatmap(\n",
    "        combined_importance,\n",
    "        top_n=30,\n",
    "        figsize=(12, 6),\n",
    "        save_path=os.path.join(save_dir, \"importance_heatmap_top30.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    # 2. 排名对比图\n",
    "    print(\"2. 生成排名对比图...\")\n",
    "    fig2, ax2 = plot_interpretability_ranking_comparison(\n",
    "        combined_importance,\n",
    "        top_n=30,\n",
    "        figsize=(14, 8),\n",
    "        save_path=os.path.join(save_dir, \"ranking_comparison_top30.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    # 3. 一致性得分热图\n",
    "    print(\"3. 生成一致性得分热图...\")\n",
    "    fig3, ax3 = plot_consensus_score_heatmap(\n",
    "        combined_importance,\n",
    "        top_n=50,\n",
    "        figsize=(14, 6),\n",
    "        save_path=os.path.join(save_dir, \"consensus_score_heatmap_top50.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    # 4. 累积重要性图\n",
    "    print(\"4. 生成累积重要性图...\")\n",
    "    fig4, ax4 = plot_cumulative_importance(\n",
    "        combined_importance,\n",
    "        top_n=50,\n",
    "        figsize=(10, 6),\n",
    "        save_path=os.path.join(save_dir, \"cumulative_importance_top50.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    # 5. 3D散点图\n",
    "    print(\"5. 生成3D散点图...\")\n",
    "    try:\n",
    "        fig5, ax5 = plot_3d_importance_scatter(\n",
    "            combined_importance,\n",
    "            top_n=50,\n",
    "            figsize=(12, 10),\n",
    "            save_path=os.path.join(save_dir, \"3d_scatter_top50.pdf\"),\n",
    "            show=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   3D图生成失败: {e}\")\n",
    "    \n",
    "    # 6. 层次聚类热图\n",
    "    print(\"6. 生成层次聚类热图...\")\n",
    "    try:\n",
    "        fig6, ax6 = plot_hierarchical_clustering_heatmap(\n",
    "            combined_importance,\n",
    "            top_n=50,\n",
    "            figsize=(14, 10),\n",
    "            save_path=os.path.join(save_dir, \"hierarchical_clustering_top50.pdf\"),\n",
    "            show=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   层次聚类图生成失败: {e}\")\n",
    "    \n",
    "    # 7. 一致性分布图\n",
    "    print(\"7. 生成一致性分布图...\")\n",
    "    fig7, axes7 = plot_consensus_distribution(\n",
    "        combined_importance,\n",
    "        top_n=100,\n",
    "        figsize=(12, 6),\n",
    "        save_path=os.path.join(save_dir, \"consensus_distribution_top100.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ 所有额外的联合分析图已生成并保存！\")\n",
    "    print(f\"保存位置: {save_dir}\")\n",
    "else:\n",
    "    print(\"⚠️ 请先运行compare_interpretability_methods函数生成combined_importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 生成总结报告\n",
    "print(\"\\n📊 三种方法一致识别的Top基因:\")\n",
    "shap_top20 = set(shap_importance.head(20)['feature'])\n",
    "lime_top20 = set(lime_importance.head(20)['feature'])\n",
    "abl_top20 = set(ablation_importance.head(20)['feature'])\n",
    "consensus_genes = shap_top20 & lime_top20 & abl_top20\n",
    "\n",
    "if consensus_genes:\n",
    "    print(f\"  三方一致 (Top 20): {sorted(consensus_genes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38376944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 绘制consensus_genes在HC和AD中的表达差异\n",
    "# ============================================================================\n",
    "\n",
    "def plot_consensus_genes_expression(\n",
    "    expr_file: str,\n",
    "    label_file: str,\n",
    "    consensus_genes: List[str],\n",
    "    sample_id_col: str = \"id\",\n",
    "    label_col: str = \"group\",\n",
    "    figsize: Tuple[int, int] = (14, 6),\n",
    "    save_path: Optional[str] = None,\n",
    "    show: bool = True\n",
    ") -> Tuple[plt.Figure, np.ndarray]:\n",
    "    \"\"\"\n",
    "    绘制consensus_genes在HC和AD中的表达差异\n",
    "    \n",
    "    Args:\n",
    "        expr_file: 表达数据文件路径\n",
    "        label_file: 标签文件路径\n",
    "        consensus_genes: 一致基因列表\n",
    "        sample_id_col: 样本ID列名\n",
    "        label_col: 标签列名\n",
    "        figsize: 图形大小\n",
    "        save_path: 保存路径\n",
    "        show: 是否显示\n",
    "    \n",
    "    Returns:\n",
    "        fig, axes: matplotlib图形对象\n",
    "    \"\"\"\n",
    "    # 1. 读取原始表达数据\n",
    "    expr_df = read_expression(expr_file)\n",
    "    labels_series = read_labels(label_file, sample_id_col, label_col)\n",
    "    \n",
    "    # 2. 对齐数据\n",
    "    expr_aligned, labels_aligned = align_expr_labels(expr_df, labels_series)\n",
    "    \n",
    "    # 3. 提取consensus_genes的表达数据\n",
    "    available_genes = [g for g in consensus_genes if g in expr_aligned.index]\n",
    "    if len(available_genes) == 0:\n",
    "        print(f\"警告: consensus_genes中没有在表达数据中找到的基因\")\n",
    "        return None, None\n",
    "    \n",
    "    if len(available_genes) < len(consensus_genes):\n",
    "        missing = set(consensus_genes) - set(available_genes)\n",
    "        print(f\"警告: 以下基因在表达数据中未找到: {missing}\")\n",
    "    \n",
    "    expr_consensus = expr_aligned.loc[available_genes].T  # 转置为 (n_samples, n_genes)\n",
    "    \n",
    "    # 4. 准备绘图数据\n",
    "    plot_data = []\n",
    "    for gene in available_genes:\n",
    "        for sample_id in expr_consensus.index:\n",
    "            label = labels_aligned.loc[sample_id]\n",
    "            label_name = \"AD\" if label == 1 else \"HC\"\n",
    "            expr_value = expr_consensus.loc[sample_id, gene]\n",
    "            plot_data.append({\n",
    "                'Gene': gene,\n",
    "                'Group': label_name,\n",
    "                'Expression': expr_value\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # 5. 绘制箱线图\n",
    "    n_genes = len(available_genes)\n",
    "    n_cols = min(4, n_genes)\n",
    "    n_rows = (n_genes + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_genes == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, gene in enumerate(available_genes):\n",
    "        ax = axes[idx]\n",
    "        gene_data = plot_df[plot_df['Gene'] == gene]\n",
    "        \n",
    "        # 箱线图\n",
    "        groups = ['HC', 'AD']\n",
    "        data_to_plot = [gene_data[gene_data['Group'] == g]['Expression'].values for g in groups]\n",
    "        \n",
    "        bp = ax.boxplot(data_to_plot, labels=groups, patch_artist=True, \n",
    "                       widths=0.6, showmeans=True, meanline=True)\n",
    "        \n",
    "        # 设置颜色\n",
    "        colors = ['#4C72B0', '#C44E52']  # HC用蓝色，AD用红色\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        # 添加散点\n",
    "        for i, group in enumerate(groups):\n",
    "            group_data = gene_data[gene_data['Group'] == group]['Expression'].values\n",
    "            x_pos = np.random.normal(i+1, 0.04, size=len(group_data))\n",
    "            ax.scatter(x_pos, group_data, alpha=0.4, s=20, color=colors[i], zorder=3)\n",
    "        \n",
    "        ax.set_title(gene, fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Expression Level', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 添加统计检验（可选）\n",
    "        from scipy import stats\n",
    "        hc_data = gene_data[gene_data['Group'] == 'HC']['Expression'].values\n",
    "        ad_data = gene_data[gene_data['Group'] == 'AD']['Expression'].values\n",
    "        if len(hc_data) > 0 and len(ad_data) > 0:\n",
    "            try:\n",
    "                stat, p_value = stats.mannwhitneyu(hc_data, ad_data, alternative='two-sided')\n",
    "                significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "                ax.text(0.5, 0.95, f'p={p_value:.3f} ({significance})', \n",
    "                       transform=ax.transAxes, ha='center', va='top',\n",
    "                       fontsize=9, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for idx in range(len(available_genes), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Expression Differences of Consensus Genes (HC vs AD)', \n",
    "                fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\", facecolor='white', edgecolor='none')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "# 绘制表达差异\n",
    "if 'consensus_genes' in locals() and len(consensus_genes) > 0:\n",
    "    fig, axes = plot_consensus_genes_expression(\n",
    "        expr_file=EXPR_FILE,\n",
    "        label_file=LABEL_FILE,\n",
    "        consensus_genes=list(consensus_genes),\n",
    "        sample_id_col=\"id\",\n",
    "        label_col=\"group\",\n",
    "        figsize=(16, 8),\n",
    "        save_path=os.path.join(OUT_DIR, \"consensus_genes_expression_difference.pdf\"),\n",
    "        show=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ 已绘制 {len(consensus_genes)} 个consensus genes的表达差异图\")\n",
    "else:\n",
    "    print(\"⚠️ 未找到consensus_genes，请先运行可解释性分析\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb55bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5e2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03335928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe77a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn121",
   "language": "python",
   "name": "gnn121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
